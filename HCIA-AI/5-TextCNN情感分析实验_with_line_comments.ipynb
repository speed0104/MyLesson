{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71487eeb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "# 导入MindSpore库\n",
    "import mindspore\n",
    "# 导入MindSpore的数据集处理模块\n",
    "import mindspore.dataset as ds\n",
    "# 导入MindSpore的神经网络模块\n",
    "import mindspore.nn as nn\n",
    "# 从MindSpore中导入Tensor类，用于处理张量计算\n",
    "from mindspore import Tensor\n",
    "# 导入MindSpore的context模块，用于设定运行环境\n",
    "from mindspore import context\n",
    "# 导入MindSpore的Model类，用于构建和训练模型\n",
    "from mindspore.train.model import Model\n",
    "# 导入MindSpore的准确率计算类\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "# 导入MindSpore的模型加载和参数加载函数\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "# 导入MindSpore的训练过程回调类，包括模型检查点保存、训练配置、损失监听和时间监听\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "# 导入MindSpore的运算操作模块\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "# 导入easydict库，用于创建类字典对象，方便通过属性而非键值对的方式访问字典元素\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# 创建一个名为cfg的配置对象，并设定相关参数\n",
    "cfg = edict({\n",
    "    'name': 'movie review',                # 配置名称\n",
    "    'pre_trained': False,                  # 是否使用预训练模型\n",
    "    'num_classes': 2,                      # 分类任务的类别数量\n",
    "    'batch_size': 64,                      # 每个批次处理的数据数量\n",
    "    'epoch_size': 4,                       # 总训练轮数\n",
    "    'weight_decay': 3e-5,                  # 权重衰减系数\n",
    "    'data_path': './data/TextCNN/data/',   # 数据集路径\n",
    "    'device_target': 'CPU',                # 设备目标，此处为CPU\n",
    "    'device_id': 0,                        # 设备ID，此处为0\n",
    "    'keep_checkpoint_max': 1,              # 保留的最大检查点数量\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',  # 检查点路径\n",
    "    'word_len': 51,                        # 单词长度\n",
    "    'vec_length': 40                       # 向量长度\n",
    "})\n",
    "\n",
    "# 设定MindSpore的运行环境，包括运行模式、设备目标和设备ID\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fb3f402",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/TextCNN/data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/TextCNN/data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91d0890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reviews:  5331\n",
      "Positive reviews:  5330\n",
      "Total reviews:  10661\n"
     ]
    }
   ],
   "source": [
    "#查看所有的评论数\n",
    "\n",
    "def count_lines(file_path):\n",
    "    # 打开文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # 使用readlines方法读取所有行，返回的是一个包含所有行的列表，通过len函数获取其长度即行数\n",
    "        num_lines = len(f.readlines())\n",
    "    return num_lines\n",
    "\n",
    "# 计算负面评论数量\n",
    "neg_reviews = count_lines(\"./data/TextCNN/data/rt-polarity.neg\")\n",
    "print(\"Negative reviews: \", neg_reviews)\n",
    "\n",
    "# 计算正面评论数量\n",
    "pos_reviews = count_lines(\"./data/TextCNN/data/rt-polarity.pos\")\n",
    "print(\"Positive reviews: \", pos_reviews)\n",
    "\n",
    "# 计算总评论数量\n",
    "total_reviews = neg_reviews + pos_reviews\n",
    "print(\"Total reviews: \", total_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13580df4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#定义数据生成类\n",
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    # 初始化函数，用于创建一个MovieReview对象\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        输入：\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "\n",
    "        # 设置影评数据的路径\n",
    "        self.path = root_dir\n",
    "\n",
    "        # 设置感情标签到数字的映射，其中\"neg\"表示消极情感，\"pos\"表示积极情感\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "\n",
    "        # 初始化一个空的文件列表\n",
    "        self.files = []\n",
    "\n",
    "        # 设置一个标志位，表示是否已经将文本转化为向量，默认为False\n",
    "        self.doConvert = False\n",
    "\n",
    "        # 使用Path库来处理路径\n",
    "        mypath = Path(self.path)\n",
    "\n",
    "        # 检查指定的路径是否存在，是否为目录，如果不是，则抛出错误\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 遍历数据目录中的文件，将找到的文件名添加到文件列表中\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 检查文件列表中是否有两个文件，即.neg文件和.pos文件，如果不是，则抛出错误\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 初始化一些参数，用于存储数据的一些统计信息\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "\n",
    "        # 初始化存储积极和消极影评的列表\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "\n",
    "        # 对文件列表中的每一个文件，调用read_data函数来读取数据\n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "\n",
    "        # 调用text2vec函数，将读取的文本数据转化为向量\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "\n",
    "        # 调用split_dataset函数，将数据集按照指定的比例分割为训练集和测试集\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "        \"\"\" 这里改为utf8编码来读，否则会报错\"\"\"\n",
    "        with open(filePath,'r', encoding='utf-8') as f:\n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "\n",
    "                # 使用空格分割句子，得到单词列表\n",
    "                sentence = sentence.split(' ')\n",
    "\n",
    "                # 使用filter函数和lambda表达式，去除单词列表中的空元素\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "\n",
    "                # 如果单词列表非空\n",
    "                if sentence:\n",
    "                    # 累加单词总数\n",
    "                    self.word_num += len(sentence)\n",
    "\n",
    "                    # 更新句子的最大长度，如果当前句子长度大于maxlen，则maxlen为当前句子长度\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "\n",
    "                    # 更新句子的最小长度，如果当前句子长度小于minlen，则minlen为当前句子长度\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "\n",
    "                    # 如果文件路径中包含'pos'，则认为当前句子的情感标签为积极，将其加入到Pos列表中\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "\n",
    "                    # 否则，认为当前句子的情感标签为消极，将其加入到Neg列表中\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    # 定义将文本转为向量的方法text2vec\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "        '''\n",
    "        # 定义词典Vocab，用于存储单词到索引的映射\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # 遍历正向评价和负向评价的列表\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            # 初始化一个长度为maxlen的向量，所有元素都为0\n",
    "            vector = [0]*maxlen\n",
    "\n",
    "            # 遍历句子中的单词以及对应的索引\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                # 如果索引超过了设定的最大长度，则停止转换\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                # 如果词典中没有当前的单词，则在词典中加入该单词，并更新向量的对应位置为词典的长度减1\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                # 如果词典中已经有当前的单词，则更新向量的对应位置为词典中单词的索引\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "\n",
    "            # 更新句子的标签为对应的向量\n",
    "            SentenceLabel[0] = vector\n",
    "        # 标记已完成转换\n",
    "        self.doConvert = True\n",
    "\n",
    "    # 定义分割数据集的方法split_dataset\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        将数据分割为训练集和测试集\n",
    "        '''\n",
    "        # 计算正向评价和负向评价应该在训练集中的数量\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "\n",
    "        # 计算需要切分的块数\n",
    "        trunk_num = int(1/(1-split))\n",
    "\n",
    "        # 初始化存储正向评价和负向评价的列表\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "\n",
    "        # 将正向评价和负向评价切分为指定数量的块，分别加入到pos_temp和neg_temp中\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "\n",
    "        # 取出第三块作为测试集\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "\n",
    "        # 剩下的块作为训练集\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        # 打乱训练集的顺序\n",
    "        random.shuffle(self.train)\n",
    "\n",
    "        \n",
    "    # 定义获取词典长度的方法get_dict_len\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获取数据集中的词汇数量，即词典的长度\n",
    "        '''\n",
    "        # 如果已经完成文本到向量的转换，返回词典的长度\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        # 如果还没有完成文本到向量的转换，打印警告信息并返回-1\n",
    "        else:\n",
    "            print(\"尚未完成文本到向量的转换\")\n",
    "            return -1\n",
    "\n",
    "    # 定义创建训练数据集的方法create_train_dataset\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        # 使用MindSpore的数据集类GeneratorDataset，将训练数据转换为数据集对象\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        # Generator是一个数据生成器，传入的参数input_list为训练数据\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        # 定义数据集的列名\n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        # 不对数据进行打乱\n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "        # 对数据集进行分批，每批的大小为batch_size，drop_remainder=True表示如果最后一批数据不足batch_size，将舍弃\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        # 对数据集进行重复，重复次数为epoch_size，这样在训练模型时可以多次使用同一数据集\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        # 返回生成的数据集\n",
    "        return dataset\n",
    "\n",
    "    # 定义创建测试数据集的方法create_test_dataset\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        # 使用MindSpore的数据集类GeneratorDataset，将测试数据转换为数据集对象\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        # Generator是一个数据生成器，传入的参数input_list为测试数据\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        # 定义数据集的列名\n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        # 不对数据进行打乱\n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "        # 对数据集进行分批，每批的大小为batch_size，drop_remainder=True表示如果最后一批数据不足batch_size，将舍弃\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        # 返回生成的数据集\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c0ff7e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 实例化MovieReview类，输入参数包括数据目录(cfg.data_path)，句子最大长度(cfg.word_len)和训练/评估的比例(0.9)\n",
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "# 调用实例的create_train_dataset方法，创建训练数据集，输入参数包括每批数据的大小(cfg.batch_size)和数据集重复的次数(cfg.epoch_size)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "# 调用数据集的get_dataset_size方法，获取数据集的大小，即批次数量\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb8a0f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[    0,  6129,    10 ...     0,     0,     0],\n",
      " [   72, 16157, 12491 ...     0,     0,     0],\n",
      " [   59,   617,   726 ...     0,     0,     0],\n",
      " ...\n",
      " [ 1176,   802,   253 ...     0,     0,     0],\n",
      " [ 5649,    97,  2233 ...     0,     0,     0],\n",
      " [  317,    97,   735 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, \n",
      " 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, \n",
      " 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0])}\n",
      "[   72 16157 12491   155 16158   305 16159    10 16160   305 13333    60\n",
      " 16161   247    15   258 16162  5320  6175    26 16163 16164   415     0\n",
      "  3873     4 16165  5142   928     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# 调用实例的get_dict_len方法，获取数据集中所有单词构成的词典的大小\n",
    "vocab_size=instance.get_dict_len()\n",
    "# 打印词汇表的大小\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "# 使用create_dict_iterator创建数据集的迭代器\n",
    "item =dataset.create_dict_iterator()\n",
    "# 遍历迭代器，并打印第一个数据项的详细信息，以及第二个数据项的数据部分\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98aef002",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化学习率列表\n",
    "learning_rate = []\n",
    "# 根据预设的参数计算预热阶段的学习率列表\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "# 根据预设的参数计算收缩阶段的学习率列表\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "# 根据预设的参数计算正常运行阶段的学习率列表\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "# 将三个阶段的学习率列表合并为一个列表\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d37c1069",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.381.550 [mindspore\\nn\\layer\\basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.387.569 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 4.80047334e-03  2.59035304e-02 -1.54866586e-02 ...  3.91998654e-03\n",
      "         6.25947537e-03 -4.53591533e-03]\n",
      "       [ 7.83338118e-03  2.35214434e-03  8.11113277e-04 ... -1.58306938e-02\n",
      "        -1.72751117e-02  2.24960013e-03]\n",
      "       [-1.45516722e-02 -4.30728355e-03  1.25398869e-02 ...  2.10527536e-02\n",
      "         3.87292297e-04 -1.05130754e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.23882908e-02 -1.24273738e-02 -1.14165852e-02 ... -1.58078149e-02\n",
      "        -1.23453392e-02 -8.79397895e-03]\n",
      "       [ 3.75452242e-03 -3.78002552e-03 -1.73468534e-02 ...  1.27675552e-02\n",
      "        -1.26821026e-02  1.20405632e-03]\n",
      "       [-6.57678582e-03 -1.77251138e-02 -5.17415442e-03 ...  5.36515327e-05\n",
      "        -1.96229317e-03  2.55728094e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.05398763e-02  7.48438528e-03 -6.20980107e-04 ...  3.82471946e-03\n",
      "         7.25340238e-03 -1.36051457e-02]\n",
      "       [ 5.63201401e-03  7.92839658e-03  5.74348541e-03 ...  1.45103391e-02\n",
      "         2.90736672e-03  1.14396438e-02]\n",
      "       [ 6.71384903e-03 -1.05531723e-03 -4.79297200e-03 ... -6.42798096e-03\n",
      "         8.54066852e-03  7.89960846e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-2.87339091e-04  3.59883858e-03  7.64440093e-03 ...  3.52862058e-03\n",
      "         1.04388557e-02 -8.72077513e-03]\n",
      "       [-8.15907214e-03  1.43972546e-04 -1.87334456e-02 ...  1.96897355e-03\n",
      "        -1.00855893e-02 -1.04402024e-02]\n",
      "       [ 5.94240613e-04  3.95503500e-03 -1.81585026e-03 ... -5.15773101e-03\n",
      "        -1.39522888e-02  8.39549024e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.92881315e-03 -1.16302054e-02  5.90849202e-03 ...  4.19465126e-03\n",
      "        -9.56402440e-03 -6.63069542e-03]\n",
      "       [-7.69161526e-03  1.29606633e-03 -1.36395358e-03 ...  1.47797149e-02\n",
      "        -1.71233509e-02  4.69713984e-03]\n",
      "       [ 3.76654789e-04  1.22525860e-02 -1.06439590e-02 ... -9.16029885e-03\n",
      "        -7.57457083e-03 -4.64163721e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-8.75094254e-03  2.26450767e-02 -4.96261695e-04 ... -1.21226115e-02\n",
      "        -1.34384967e-02 -1.29579837e-02]\n",
      "       [ 3.00631463e-03  1.12406928e-02  8.33529793e-03 ... -9.27290646e-04\n",
      "         1.56134618e-02  5.19659510e-03]\n",
      "       [-8.61817319e-03  1.51434420e-02 -4.52503562e-03 ...  1.47657655e-02\n",
      "        -1.27054211e-02 -2.89041176e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-5.36181452e-03 -3.55778774e-03  1.05976425e-02 ... -7.56116072e-03\n",
      "        -2.46647134e-04  1.04183494e-03]\n",
      "       [ 7.43453857e-03  1.41332848e-02  4.38180607e-04 ... -9.07927658e-03\n",
      "         9.27846972e-03  2.63174181e-03]\n",
      "       [-1.00311209e-02 -6.16898388e-03 -9.77017824e-03 ...  1.16197960e-02\n",
      "         1.30687607e-02 -1.49549227e-02]\n",
      "       [ 7.18373992e-03  5.08597074e-03  2.22013029e-03 ...  6.39188522e-03\n",
      "        -8.01710971e-03 -1.02235738e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.92831620e-03 -6.28762459e-03  1.63706485e-02 ... -2.95419269e-03\n",
      "        -8.03446677e-03 -1.48068694e-02]\n",
      "       [ 1.28963087e-02 -1.53047452e-03  4.97132167e-03 ... -5.02116000e-03\n",
      "        -6.30905386e-04  9.54070769e-04]\n",
      "       [ 1.19507732e-02 -3.66206490e-03  5.84640889e-04 ... -5.77832339e-03\n",
      "        -1.79378372e-02  6.63707731e-03]\n",
      "       [ 2.29517906e-03  1.50824543e-02  7.41367182e-03 ... -7.80360028e-03\n",
      "        -5.80632780e-03 -9.92819667e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.03375763e-02 -4.44637239e-03  8.74225632e-04 ... -1.50327794e-02\n",
      "         1.70431286e-02 -1.11805077e-03]\n",
      "       [ 4.03552689e-03  9.62321367e-03  6.95856847e-03 ... -7.31216511e-03\n",
      "        -1.09728873e-02  5.96836675e-03]\n",
      "       [ 3.90651112e-04  7.89903352e-05 -1.04968315e-02 ... -1.10189512e-03\n",
      "         2.89960634e-02 -1.75309513e-04]\n",
      "       [-1.75927263e-02  4.42002289e-04 -5.06539305e-04 ...  4.16501367e-04\n",
      "        -3.39147868e-03 -7.96407368e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 6.05221000e-03  9.93297342e-03 -9.78254434e-03 ...  1.35190698e-04\n",
      "         6.64713886e-03 -1.92634258e-02]\n",
      "       [-6.91762660e-03 -1.16063906e-02  2.31050085e-02 ... -4.63857362e-03\n",
      "        -1.14906272e-02 -1.08368164e-02]\n",
      "       [ 1.33930184e-02 -1.02452524e-02 -3.74408346e-03 ... -3.00599681e-03\n",
      "        -5.65967197e-03 -1.60085801e-02]\n",
      "       [ 2.30444018e-02 -2.04954273e-03 -4.24363231e-03 ... -6.46573084e-04\n",
      "         1.09773176e-02 -8.00873712e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.23737091e-02  1.47567624e-02 -9.35053639e-03 ... -1.01549048e-02\n",
      "        -1.68338828e-02 -7.07634678e-03]\n",
      "       [-7.19978940e-04 -1.68646015e-02  1.33246393e-03 ... -1.27052115e-02\n",
      "        -4.09117958e-04  1.30353915e-02]\n",
      "       [ 1.04166614e-03 -1.99069898e-03 -8.52242485e-03 ... -1.92072857e-02\n",
      "        -1.16390390e-02 -6.30185753e-03]\n",
      "       [ 1.61398184e-02 -4.27804654e-03  4.63915896e-03 ...  1.00682769e-02\n",
      "        -2.62014172e-03 -5.88218635e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 2.98457290e-03  1.07699716e-02 -1.63329262e-02 ...  5.26632415e-03\n",
      "         1.35558397e-02  4.00322536e-03]\n",
      "       [ 1.88315026e-02  2.80943210e-03 -7.07819220e-03 ...  3.49393405e-04\n",
      "         1.23495134e-02 -3.01724928e-03]\n",
      "       [-1.41812640e-03  1.97599065e-02 -3.76631762e-03 ... -6.27738237e-03\n",
      "         3.10124917e-04 -4.31291293e-03]\n",
      "       [-3.99080757e-03 -1.98819991e-02  1.04098592e-03 ...  4.71265661e-03\n",
      "        -8.21398385e-03 -5.36665646e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.81511641e-02  1.84986591e-02 -5.08242613e-03 ...  9.10089444e-03\n",
      "         2.80791661e-03  2.63697421e-03]\n",
      "       [-1.61194541e-02  1.50193982e-02  5.08484431e-03 ...  1.13834506e-02\n",
      "        -1.47223175e-02 -8.77705123e-03]\n",
      "       [-1.15609523e-02 -8.88526440e-03  9.88630205e-03 ... -5.10163512e-03\n",
      "         4.28961078e-03  4.09193849e-03]\n",
      "       [ 2.63337716e-02 -2.48866417e-02  7.35923462e-03 ...  3.55317199e-04\n",
      "        -8.58305022e-04  1.55329769e-02]\n",
      "       [-1.59565476e-03 -2.61709763e-04  7.37041142e-03 ... -3.09684244e-03\n",
      "         1.02617806e-02 -1.16910124e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 6.20011613e-03 -2.84774415e-03 -1.09004695e-02 ... -1.79633368e-02\n",
      "         1.28665539e-02  1.01677179e-02]\n",
      "       [ 5.38784359e-03  6.67591020e-03 -1.40440254e-03 ... -8.08669161e-03\n",
      "         6.00347354e-04  1.39418722e-03]\n",
      "       [ 4.56590758e-04  2.96728965e-03 -1.15597807e-02 ...  8.49891361e-03\n",
      "        -2.38375068e-02 -2.00289534e-03]\n",
      "       [ 1.89609006e-02  5.91122080e-03  2.03203014e-03 ...  1.59164274e-03\n",
      "        -3.82913114e-03  3.21414601e-03]\n",
      "       [ 9.53846052e-03  1.12015447e-02  1.23705659e-02 ... -7.50587648e-03\n",
      "         2.14367267e-02 -9.70464852e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.44387898e-03  1.15012610e-02  8.01747292e-03 ... -7.54086953e-03\n",
      "         1.00877676e-02  9.22786910e-03]\n",
      "       [ 4.87162312e-03 -9.03599575e-05 -3.55742709e-03 ... -6.99776877e-03\n",
      "         8.98335502e-03 -3.89217865e-03]\n",
      "       [-1.01493662e-02  1.68716870e-02 -7.05664838e-03 ...  7.81494286e-03\n",
      "         2.96002510e-03 -6.36316603e-03]\n",
      "       [ 5.07515436e-03 -5.60588844e-04 -6.20865216e-03 ... -5.47732599e-03\n",
      "        -9.02786385e-03  1.44407805e-02]\n",
      "       [-1.80880520e-02 -4.83248616e-03  1.61356889e-02 ... -1.65323690e-02\n",
      "         2.08438095e-03 -3.44995409e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 5.96180675e-04  2.57798238e-03 -9.49655659e-03 ... -2.82579544e-03\n",
      "        -2.85198260e-03  1.19581597e-03]\n",
      "       [ 2.14035506e-03  3.45611107e-03  3.41910054e-03 ... -1.16902543e-02\n",
      "         2.17552856e-02  1.48763545e-02]\n",
      "       [-1.19473375e-02  6.20010681e-03 -1.83373261e-02 ... -7.62955612e-03\n",
      "        -1.28364656e-02  4.06898884e-03]\n",
      "       [-1.82003248e-02  3.28719500e-03 -7.43786630e-04 ... -1.17010355e-03\n",
      "         4.23465250e-03  2.75731529e-03]\n",
      "       [-7.89114367e-03  6.17237901e-03  6.58307178e-03 ... -4.91498178e-03\n",
      "         2.22509983e-03 -1.28371827e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.49699044e-03  9.28773824e-03  1.14561559e-03 ...  3.60426959e-04\n",
      "         4.24018162e-06 -9.85128060e-03]\n",
      "       [-3.59408022e-03 -1.52894156e-03 -1.31596457e-02 ... -8.16261396e-03\n",
      "         6.18621055e-03  4.64906078e-03]\n",
      "       [ 1.45636185e-03  2.03841235e-02 -1.52298836e-02 ... -9.87129752e-03\n",
      "         1.13084994e-03 -1.40267992e-02]\n",
      "       [-2.23662591e-05  1.48392040e-02  1.13708377e-02 ...  3.82864918e-03\n",
      "        -2.33647437e-03  1.80570297e-02]\n",
      "       [-5.46027906e-03  2.45135352e-02  1.57942157e-02 ... -1.28777290e-03\n",
      "        -2.21716496e-03 -4.85328585e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.63665563e-03  2.74448991e-02  2.94592399e-02 ...  3.63129308e-03\n",
      "        -2.77246791e-03 -6.23528031e-04]\n",
      "       [ 1.13480154e-03 -2.63946354e-02 -1.03187962e-02 ... -1.49689186e-02\n",
      "         3.92216584e-03  2.73841973e-02]\n",
      "       [ 6.20152708e-03 -1.09723471e-02  1.59156192e-02 ... -5.15088346e-03\n",
      "         1.92449074e-02  7.99180381e-03]\n",
      "       [-3.38876201e-03  8.40280764e-03  3.16364761e-03 ... -5.39691420e-04\n",
      "         3.26629519e-03  4.57616849e-03]\n",
      "       [ 9.65553150e-03  1.96209596e-03  9.68992431e-03 ...  8.93298257e-03\n",
      "        -1.02080954e-02  4.13748715e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，用于创建模型权重，输入参数为权重的形状和一个缩放因子\n",
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor  # 使用随机正态分布生成初始权重，然后乘以缩放因子\n",
    "    return Tensor(init_value)  # 将numpy数组转为Tensor并返回\n",
    "\n",
    "# 定义一个函数，用于创建卷积层，输入参数为卷积核的大小\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)  # 定义权重的形状\n",
    "    weight = _weight_variable(weight_shape)  # 调用前面定义的函数，创建权重\n",
    "    # 创建卷积层，输入通道为1，输出通道为96，卷积核大小为输入参数，边界填充大小为1，权重初始化为前面创建的权重，有偏置\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "# 定义TextCNN模型，继承自nn.Cell\n",
    "class TextCNN(nn.Cell):\n",
    "    # 构造函数，输入参数为词汇量、词长、类别数和向量长度\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()  # 调用父类的构造函数\n",
    "        self.vec_length = vec_length  # 设置向量长度\n",
    "        self.word_len = word_len  # 设置词长\n",
    "        self.num_classes = num_classes  # 设置类别数\n",
    "\n",
    "        # 初始化所需的各种操作和层\n",
    "        self.unsqueeze = ops.ExpandDims()  # 初始化一个用于扩展维度的操作\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')  # 初始化一个嵌入层\n",
    "\n",
    "        self.slice = ops.Slice()  # 初始化一个用于切片的操作\n",
    "        # 初始化三个卷积层，分别使用3、4、5的高度的卷积核\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)  # 初始化一个用于在第一维度上连接张量的操作\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)  # 初始化一个全连接层，输入尺寸为96*3，输出尺寸为类别数\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)  # 初始化一个Dropout层，保持概率为0.5\n",
    "        self.print = ops.Print()  # 初始化一个用于打印的操作\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)  # 初始化一个用于在不保持维度的情况下进行最大值降维的操作\n",
    "    \n",
    "    # 定义一个函数，用于创建一个卷积层、ReLU激活层和最大池化层的序列模型\n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),  # 调用前面定义的函数，创建卷积层\n",
    "                nn.ReLU(),  # 创建ReLU激活层\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),  # 创建最大池化层，核的大小为(词长-卷积核高度+1,1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # 定义模型的前向传播\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)  # 扩展输入x的维度\n",
    "        x = self.embedding(x)  # 将输入x传入嵌入层得到输出\n",
    "        x1 = self.layer1(x)  # 将x传入第一个卷积层得到输出x1\n",
    "        x2 = self.layer2(x)  # 将x传入第二个卷积层得到输出x2\n",
    "        x3 = self.layer3(x)  # 将x传入第三个卷积层得到输出x3\n",
    "\n",
    "        # 对x1, x2, x3进行降维操作\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))  # 将x1, x2, x3在第一维度上连接\n",
    "        x = self.drop(x)  # 对连接后的x执行Dropout操作\n",
    "        x = self.fc(x)  # 将Dropout后的x传入全连接层得到最终的输出\n",
    "        return x  # 返回最终的输出\n",
    "\n",
    "# 实例化一个TextCNN模型，词汇量、词长、类别数和向量长度根据先前设定的参数进行设置\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)\n",
    "print(net)  # 打印模型的结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "310eb3bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 优化器、损失函数、保存检查点、时间监视器等设置\n",
    "# 使用Adam优化器，过滤出网络中需要梯度的参数，设置学习率和权重衰减\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "# 设置损失函数为带有对数似然的softmax交叉熵，设定标签为稀疏\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "# 创建模型，输入网络、损失函数、优化器和度量标准\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n",
    "\n",
    "# 配置模型保存的配置，设定保存模型的步骤和最大保存的模型数量\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), \n",
    "                             keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "\n",
    "# 创建时间监控回调函数，设定数据大小\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "\n",
    "# 设定模型保存路径\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "# 创建模型保存的回调函数，设置保存的前缀、路径和配置\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "\n",
    "# 创建损失监控的回调函数\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa0a9467",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.457.990 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.466.252 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.483.018 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.489.132 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(22732:700,MainProcess):2023-07-17-09:57:54.520.364 [mindspore\\nn\\layer\\basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 0.6931294202804565\n",
      "epoch: 1 step: 2, loss is 0.6930627822875977\n",
      "epoch: 1 step: 3, loss is 0.6930456757545471\n",
      "epoch: 1 step: 4, loss is 0.6927811503410339\n",
      "epoch: 1 step: 5, loss is 0.6926908493041992\n",
      "epoch: 1 step: 6, loss is 0.6931170225143433\n",
      "epoch: 1 step: 7, loss is 0.6937335729598999\n",
      "epoch: 1 step: 8, loss is 0.693136990070343\n",
      "epoch: 1 step: 9, loss is 0.6941984295845032\n",
      "epoch: 1 step: 10, loss is 0.6942269802093506\n",
      "epoch: 1 step: 11, loss is 0.6923094391822815\n",
      "epoch: 1 step: 12, loss is 0.6960406303405762\n",
      "epoch: 1 step: 13, loss is 0.6962171792984009\n",
      "epoch: 1 step: 14, loss is 0.6948647499084473\n",
      "epoch: 1 step: 15, loss is 0.6928861737251282\n",
      "epoch: 1 step: 16, loss is 0.6932189464569092\n",
      "epoch: 1 step: 17, loss is 0.6926476955413818\n",
      "epoch: 1 step: 18, loss is 0.693326473236084\n",
      "epoch: 1 step: 19, loss is 0.6929855942726135\n",
      "epoch: 1 step: 20, loss is 0.6933774352073669\n",
      "epoch: 1 step: 21, loss is 0.6929882764816284\n",
      "epoch: 1 step: 22, loss is 0.6932856440544128\n",
      "epoch: 1 step: 23, loss is 0.6930728554725647\n",
      "epoch: 1 step: 24, loss is 0.6929888725280762\n",
      "epoch: 1 step: 25, loss is 0.6934244632720947\n",
      "epoch: 1 step: 26, loss is 0.6928998231887817\n",
      "epoch: 1 step: 27, loss is 0.6938846111297607\n",
      "epoch: 1 step: 28, loss is 0.6920057535171509\n",
      "epoch: 1 step: 29, loss is 0.6934021711349487\n",
      "epoch: 1 step: 30, loss is 0.6930257081985474\n",
      "epoch: 1 step: 31, loss is 0.693283200263977\n",
      "epoch: 1 step: 32, loss is 0.6919608116149902\n",
      "epoch: 1 step: 33, loss is 0.6934095621109009\n",
      "epoch: 1 step: 34, loss is 0.6944740414619446\n",
      "epoch: 1 step: 35, loss is 0.6920803189277649\n",
      "epoch: 1 step: 36, loss is 0.6926030516624451\n",
      "epoch: 1 step: 37, loss is 0.6924843192100525\n",
      "epoch: 1 step: 38, loss is 0.6935055255889893\n",
      "epoch: 1 step: 39, loss is 0.6928923726081848\n",
      "epoch: 1 step: 40, loss is 0.6936677694320679\n",
      "epoch: 1 step: 41, loss is 0.6895462274551392\n",
      "epoch: 1 step: 42, loss is 0.6917318105697632\n",
      "epoch: 1 step: 43, loss is 0.6930339336395264\n",
      "epoch: 1 step: 44, loss is 0.6923239231109619\n",
      "epoch: 1 step: 45, loss is 0.6936349272727966\n",
      "epoch: 1 step: 46, loss is 0.6923577189445496\n",
      "epoch: 1 step: 47, loss is 0.6938814520835876\n",
      "epoch: 1 step: 48, loss is 0.6905530691146851\n",
      "epoch: 1 step: 49, loss is 0.6988725066184998\n",
      "epoch: 1 step: 50, loss is 0.6918408870697021\n",
      "epoch: 1 step: 51, loss is 0.690830409526825\n",
      "epoch: 1 step: 52, loss is 0.6915324926376343\n",
      "epoch: 1 step: 53, loss is 0.6914556622505188\n",
      "epoch: 1 step: 54, loss is 0.6936554908752441\n",
      "epoch: 1 step: 55, loss is 0.6923285722732544\n",
      "epoch: 1 step: 56, loss is 0.6926106214523315\n",
      "epoch: 1 step: 57, loss is 0.6917358636856079\n",
      "epoch: 1 step: 58, loss is 0.6953437328338623\n",
      "epoch: 1 step: 59, loss is 0.690474271774292\n",
      "epoch: 1 step: 60, loss is 0.6894205212593079\n",
      "epoch: 1 step: 61, loss is 0.6926446557044983\n",
      "epoch: 1 step: 62, loss is 0.6935614347457886\n",
      "epoch: 1 step: 63, loss is 0.6931824088096619\n",
      "epoch: 1 step: 64, loss is 0.6987918615341187\n",
      "epoch: 1 step: 65, loss is 0.690269947052002\n",
      "epoch: 1 step: 66, loss is 0.692855954170227\n",
      "epoch: 1 step: 67, loss is 0.6924852728843689\n",
      "epoch: 1 step: 68, loss is 0.6932265758514404\n",
      "epoch: 1 step: 69, loss is 0.689739465713501\n",
      "epoch: 1 step: 70, loss is 0.6921833753585815\n",
      "epoch: 1 step: 71, loss is 0.6921361088752747\n",
      "epoch: 1 step: 72, loss is 0.6898915767669678\n",
      "epoch: 1 step: 73, loss is 0.6871462464332581\n",
      "epoch: 1 step: 74, loss is 0.688775897026062\n",
      "epoch: 1 step: 75, loss is 0.688273549079895\n",
      "epoch: 1 step: 76, loss is 0.689499020576477\n",
      "epoch: 1 step: 77, loss is 0.6901934146881104\n",
      "epoch: 1 step: 78, loss is 0.6897099018096924\n",
      "epoch: 1 step: 79, loss is 0.6915415525436401\n",
      "epoch: 1 step: 80, loss is 0.6873736381530762\n",
      "epoch: 1 step: 81, loss is 0.6873997449874878\n",
      "epoch: 1 step: 82, loss is 0.6885600686073303\n",
      "epoch: 1 step: 83, loss is 0.6903437376022339\n",
      "epoch: 1 step: 84, loss is 0.6897603869438171\n",
      "epoch: 1 step: 85, loss is 0.682338535785675\n",
      "epoch: 1 step: 86, loss is 0.6789069771766663\n",
      "epoch: 1 step: 87, loss is 0.6709421277046204\n",
      "epoch: 1 step: 88, loss is 0.6616849899291992\n",
      "epoch: 1 step: 89, loss is 0.6832281351089478\n",
      "epoch: 1 step: 90, loss is 0.6790744066238403\n",
      "epoch: 1 step: 91, loss is 0.6791909337043762\n",
      "epoch: 1 step: 92, loss is 0.6792149543762207\n",
      "epoch: 1 step: 93, loss is 0.6668886542320251\n",
      "epoch: 1 step: 94, loss is 0.6812012791633606\n",
      "epoch: 1 step: 95, loss is 0.6652853488922119\n",
      "epoch: 1 step: 96, loss is 0.6735630035400391\n",
      "epoch: 1 step: 97, loss is 0.6775874495506287\n",
      "epoch: 1 step: 98, loss is 0.6688404679298401\n",
      "epoch: 1 step: 99, loss is 0.6743687987327576\n",
      "epoch: 1 step: 100, loss is 0.6698412895202637\n",
      "epoch: 1 step: 101, loss is 0.6706653833389282\n",
      "epoch: 1 step: 102, loss is 0.6713486313819885\n",
      "epoch: 1 step: 103, loss is 0.6396546959877014\n",
      "epoch: 1 step: 104, loss is 0.6619659662246704\n",
      "epoch: 1 step: 105, loss is 0.6444662809371948\n",
      "epoch: 1 step: 106, loss is 0.6504569053649902\n",
      "epoch: 1 step: 107, loss is 0.6277777552604675\n",
      "epoch: 1 step: 108, loss is 0.63847815990448\n",
      "epoch: 1 step: 109, loss is 0.6137198209762573\n",
      "epoch: 1 step: 110, loss is 0.6316120028495789\n",
      "epoch: 1 step: 111, loss is 0.6374657154083252\n",
      "epoch: 1 step: 112, loss is 0.5827219486236572\n",
      "epoch: 1 step: 113, loss is 0.63053297996521\n",
      "epoch: 1 step: 114, loss is 0.608148455619812\n",
      "epoch: 1 step: 115, loss is 0.6006882786750793\n",
      "epoch: 1 step: 116, loss is 0.5836296081542969\n",
      "epoch: 1 step: 117, loss is 0.570401132106781\n",
      "epoch: 1 step: 118, loss is 0.6409862041473389\n",
      "epoch: 1 step: 119, loss is 0.6771975755691528\n",
      "epoch: 1 step: 120, loss is 0.6207870244979858\n",
      "epoch: 1 step: 121, loss is 0.5685776472091675\n",
      "epoch: 1 step: 122, loss is 0.5935182571411133\n",
      "epoch: 1 step: 123, loss is 0.5960218906402588\n",
      "epoch: 1 step: 124, loss is 0.4824846386909485\n",
      "epoch: 1 step: 125, loss is 0.5942744612693787\n",
      "epoch: 1 step: 126, loss is 0.62557452917099\n",
      "epoch: 1 step: 127, loss is 0.5577867031097412\n",
      "epoch: 1 step: 128, loss is 0.5479991436004639\n",
      "epoch: 1 step: 129, loss is 0.5878185629844666\n",
      "epoch: 1 step: 130, loss is 0.5773420333862305\n",
      "epoch: 1 step: 131, loss is 0.5553514957427979\n",
      "epoch: 1 step: 132, loss is 0.6561463475227356\n",
      "epoch: 1 step: 133, loss is 0.5883527994155884\n",
      "epoch: 1 step: 134, loss is 0.49776142835617065\n",
      "epoch: 1 step: 135, loss is 0.5709057450294495\n",
      "epoch: 1 step: 136, loss is 0.5731099247932434\n",
      "epoch: 1 step: 137, loss is 0.5580229759216309\n",
      "epoch: 1 step: 138, loss is 0.6214276552200317\n",
      "epoch: 1 step: 139, loss is 0.5702139139175415\n",
      "epoch: 1 step: 140, loss is 0.6470926403999329\n",
      "epoch: 1 step: 141, loss is 0.5488443374633789\n",
      "epoch: 1 step: 142, loss is 0.4875812232494354\n",
      "epoch: 1 step: 143, loss is 0.5559068322181702\n",
      "epoch: 1 step: 144, loss is 0.5995030403137207\n",
      "epoch: 1 step: 145, loss is 0.51781165599823\n",
      "epoch: 1 step: 146, loss is 0.5750246047973633\n",
      "epoch: 1 step: 147, loss is 0.5389296412467957\n",
      "epoch: 1 step: 148, loss is 0.5455949306488037\n",
      "epoch: 1 step: 149, loss is 0.4763725996017456\n",
      "epoch: 1 step: 150, loss is 0.5542981624603271\n",
      "epoch: 1 step: 151, loss is 0.5551620125770569\n",
      "epoch: 1 step: 152, loss is 0.4786936342716217\n",
      "epoch: 1 step: 153, loss is 0.43874824047088623\n",
      "epoch: 1 step: 154, loss is 0.5177013278007507\n",
      "epoch: 1 step: 155, loss is 0.5852789282798767\n",
      "epoch: 1 step: 156, loss is 0.5312461256980896\n",
      "epoch: 1 step: 157, loss is 0.45464909076690674\n",
      "epoch: 1 step: 158, loss is 0.5182682275772095\n",
      "epoch: 1 step: 159, loss is 0.5386427044868469\n",
      "epoch: 1 step: 160, loss is 0.578612208366394\n",
      "epoch: 1 step: 161, loss is 0.4893818795681\n",
      "epoch: 1 step: 162, loss is 0.5186921954154968\n",
      "epoch: 1 step: 163, loss is 0.4854709804058075\n",
      "epoch: 1 step: 164, loss is 0.5866313576698303\n",
      "epoch: 1 step: 165, loss is 0.418667733669281\n",
      "epoch: 1 step: 166, loss is 0.6150508522987366\n",
      "epoch: 1 step: 167, loss is 0.4878253936767578\n",
      "epoch: 1 step: 168, loss is 0.4409712851047516\n",
      "epoch: 1 step: 169, loss is 0.4663270115852356\n",
      "epoch: 1 step: 170, loss is 0.4315654933452606\n",
      "epoch: 1 step: 171, loss is 0.44213658571243286\n",
      "epoch: 1 step: 172, loss is 0.553166389465332\n",
      "epoch: 1 step: 173, loss is 0.4515048861503601\n",
      "epoch: 1 step: 174, loss is 0.6025906205177307\n",
      "epoch: 1 step: 175, loss is 0.49489110708236694\n",
      "epoch: 1 step: 176, loss is 0.5441451072692871\n",
      "epoch: 1 step: 177, loss is 0.6092215776443481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 178, loss is 0.48210620880126953\n",
      "epoch: 1 step: 179, loss is 0.6528003811836243\n",
      "epoch: 1 step: 180, loss is 0.46307671070098877\n",
      "epoch: 1 step: 181, loss is 0.4863908290863037\n",
      "epoch: 1 step: 182, loss is 0.535204291343689\n",
      "epoch: 1 step: 183, loss is 0.422556608915329\n",
      "epoch: 1 step: 184, loss is 0.5165353417396545\n",
      "epoch: 1 step: 185, loss is 0.4886051118373871\n",
      "epoch: 1 step: 186, loss is 0.4545761048793793\n",
      "epoch: 1 step: 187, loss is 0.5248510837554932\n",
      "epoch: 1 step: 188, loss is 0.4569717347621918\n",
      "epoch: 1 step: 189, loss is 0.5152373313903809\n",
      "epoch: 1 step: 190, loss is 0.4210676848888397\n",
      "epoch: 1 step: 191, loss is 0.6214286088943481\n",
      "epoch: 1 step: 192, loss is 0.47772079706192017\n",
      "epoch: 1 step: 193, loss is 0.44909271597862244\n",
      "epoch: 1 step: 194, loss is 0.4589487910270691\n",
      "epoch: 1 step: 195, loss is 0.5437012910842896\n",
      "epoch: 1 step: 196, loss is 0.48615312576293945\n",
      "epoch: 1 step: 197, loss is 0.3743930757045746\n",
      "epoch: 1 step: 198, loss is 0.4763217270374298\n",
      "epoch: 1 step: 199, loss is 0.46000754833221436\n",
      "epoch: 1 step: 200, loss is 0.5172342658042908\n",
      "epoch: 1 step: 201, loss is 0.43374261260032654\n",
      "epoch: 1 step: 202, loss is 0.48983168601989746\n",
      "epoch: 1 step: 203, loss is 0.4461110830307007\n",
      "epoch: 1 step: 204, loss is 0.48143982887268066\n",
      "epoch: 1 step: 205, loss is 0.34657350182533264\n",
      "epoch: 1 step: 206, loss is 0.47048813104629517\n",
      "epoch: 1 step: 207, loss is 0.4196832478046417\n",
      "epoch: 1 step: 208, loss is 0.4115638732910156\n",
      "epoch: 1 step: 209, loss is 0.4122820198535919\n",
      "epoch: 1 step: 210, loss is 0.4102414846420288\n",
      "epoch: 1 step: 211, loss is 0.43635356426239014\n",
      "epoch: 1 step: 212, loss is 0.41019558906555176\n",
      "epoch: 1 step: 213, loss is 0.5465016961097717\n",
      "epoch: 1 step: 214, loss is 0.43661069869995117\n",
      "epoch: 1 step: 215, loss is 0.4596197307109833\n",
      "epoch: 1 step: 216, loss is 0.4212372899055481\n",
      "epoch: 1 step: 217, loss is 0.4876956343650818\n",
      "epoch: 1 step: 218, loss is 0.4262144863605499\n",
      "epoch: 1 step: 219, loss is 0.45943915843963623\n",
      "epoch: 1 step: 220, loss is 0.42781105637550354\n",
      "epoch: 1 step: 221, loss is 0.4585517644882202\n",
      "epoch: 1 step: 222, loss is 0.31400975584983826\n",
      "epoch: 1 step: 223, loss is 0.4531499147415161\n",
      "epoch: 1 step: 224, loss is 0.43993011116981506\n",
      "epoch: 1 step: 225, loss is 0.38145434856414795\n",
      "epoch: 1 step: 226, loss is 0.4483621418476105\n",
      "epoch: 1 step: 227, loss is 0.4250871241092682\n",
      "epoch: 1 step: 228, loss is 0.38099950551986694\n",
      "epoch: 1 step: 229, loss is 0.4254385828971863\n",
      "epoch: 1 step: 230, loss is 0.4047296643257141\n",
      "epoch: 1 step: 231, loss is 0.2608436942100525\n",
      "epoch: 1 step: 232, loss is 0.4369943141937256\n",
      "epoch: 1 step: 233, loss is 0.4418860971927643\n",
      "epoch: 1 step: 234, loss is 0.43064022064208984\n",
      "epoch: 1 step: 235, loss is 0.4069439768791199\n",
      "epoch: 1 step: 236, loss is 0.3887230455875397\n",
      "epoch: 1 step: 237, loss is 0.312080055475235\n",
      "epoch: 1 step: 238, loss is 0.4394456744194031\n",
      "epoch: 1 step: 239, loss is 0.4372893273830414\n",
      "epoch: 1 step: 240, loss is 0.41656407713890076\n",
      "epoch: 1 step: 241, loss is 0.34882307052612305\n",
      "epoch: 1 step: 242, loss is 0.275968074798584\n",
      "epoch: 1 step: 243, loss is 0.37288784980773926\n",
      "epoch: 1 step: 244, loss is 0.3784900903701782\n",
      "epoch: 1 step: 245, loss is 0.4768916964530945\n",
      "epoch: 1 step: 246, loss is 0.36999744176864624\n",
      "epoch: 1 step: 247, loss is 0.3061392605304718\n",
      "epoch: 1 step: 248, loss is 0.3166888654232025\n",
      "epoch: 1 step: 249, loss is 0.3705962896347046\n",
      "epoch: 1 step: 250, loss is 0.4138639569282532\n",
      "epoch: 1 step: 251, loss is 0.3726130723953247\n",
      "epoch: 1 step: 252, loss is 0.3613877594470978\n",
      "epoch: 1 step: 253, loss is 0.3314107060432434\n",
      "epoch: 1 step: 254, loss is 0.3233150243759155\n",
      "epoch: 1 step: 255, loss is 0.28169137239456177\n",
      "epoch: 1 step: 256, loss is 0.31945809721946716\n",
      "epoch: 1 step: 257, loss is 0.3049311935901642\n",
      "epoch: 1 step: 258, loss is 0.2724868357181549\n",
      "epoch: 1 step: 259, loss is 0.35964101552963257\n",
      "epoch: 1 step: 260, loss is 0.3252354860305786\n",
      "epoch: 1 step: 261, loss is 0.24526923894882202\n",
      "epoch: 1 step: 262, loss is 0.39637985825538635\n",
      "epoch: 1 step: 263, loss is 0.27999669313430786\n",
      "epoch: 1 step: 264, loss is 0.2407110035419464\n",
      "epoch: 1 step: 265, loss is 0.25775983929634094\n",
      "epoch: 1 step: 266, loss is 0.2732645273208618\n",
      "epoch: 1 step: 267, loss is 0.39045923948287964\n",
      "epoch: 1 step: 268, loss is 0.5121878981590271\n",
      "epoch: 1 step: 269, loss is 0.34229138493537903\n",
      "epoch: 1 step: 270, loss is 0.26337194442749023\n",
      "epoch: 1 step: 271, loss is 0.33587828278541565\n",
      "epoch: 1 step: 272, loss is 0.35112717747688293\n",
      "epoch: 1 step: 273, loss is 0.18074144423007965\n",
      "epoch: 1 step: 274, loss is 0.3404405117034912\n",
      "epoch: 1 step: 275, loss is 0.431231290102005\n",
      "epoch: 1 step: 276, loss is 0.301003634929657\n",
      "epoch: 1 step: 277, loss is 0.37927860021591187\n",
      "epoch: 1 step: 278, loss is 0.2628943622112274\n",
      "epoch: 1 step: 279, loss is 0.3326364755630493\n",
      "epoch: 1 step: 280, loss is 0.2483111321926117\n",
      "epoch: 1 step: 281, loss is 0.37195613980293274\n",
      "epoch: 1 step: 282, loss is 0.2868405282497406\n",
      "epoch: 1 step: 283, loss is 0.24945968389511108\n",
      "epoch: 1 step: 284, loss is 0.22719168663024902\n",
      "epoch: 1 step: 285, loss is 0.28304624557495117\n",
      "epoch: 1 step: 286, loss is 0.37230634689331055\n",
      "epoch: 1 step: 287, loss is 0.34283965826034546\n",
      "epoch: 1 step: 288, loss is 0.32107222080230713\n",
      "epoch: 1 step: 289, loss is 0.3300527334213257\n",
      "epoch: 1 step: 290, loss is 0.3112359046936035\n",
      "epoch: 1 step: 291, loss is 0.2129969298839569\n",
      "epoch: 1 step: 292, loss is 0.2783275246620178\n",
      "epoch: 1 step: 293, loss is 0.2999727129936218\n",
      "epoch: 1 step: 294, loss is 0.22857342660427094\n",
      "epoch: 1 step: 295, loss is 0.38986676931381226\n",
      "epoch: 1 step: 296, loss is 0.3460397720336914\n",
      "epoch: 1 step: 297, loss is 0.22200104594230652\n",
      "epoch: 1 step: 298, loss is 0.19130939245224\n",
      "epoch: 1 step: 299, loss is 0.3340645432472229\n",
      "epoch: 1 step: 300, loss is 0.33303385972976685\n",
      "epoch: 1 step: 301, loss is 0.25354117155075073\n",
      "epoch: 1 step: 302, loss is 0.264367938041687\n",
      "epoch: 1 step: 303, loss is 0.28637540340423584\n",
      "epoch: 1 step: 304, loss is 0.2679152488708496\n",
      "epoch: 1 step: 305, loss is 0.27141305804252625\n",
      "epoch: 1 step: 306, loss is 0.14815481007099152\n",
      "epoch: 1 step: 307, loss is 0.27988290786743164\n",
      "epoch: 1 step: 308, loss is 0.28726255893707275\n",
      "epoch: 1 step: 309, loss is 0.37532296776771545\n",
      "epoch: 1 step: 310, loss is 0.22306159138679504\n",
      "epoch: 1 step: 311, loss is 0.32975512742996216\n",
      "epoch: 1 step: 312, loss is 0.24101755023002625\n",
      "epoch: 1 step: 313, loss is 0.31488773226737976\n",
      "epoch: 1 step: 314, loss is 0.16336487233638763\n",
      "epoch: 1 step: 315, loss is 0.3673289120197296\n",
      "epoch: 1 step: 316, loss is 0.292822003364563\n",
      "epoch: 1 step: 317, loss is 0.22094106674194336\n",
      "epoch: 1 step: 318, loss is 0.22948205471038818\n",
      "epoch: 1 step: 319, loss is 0.2240561991930008\n",
      "epoch: 1 step: 320, loss is 0.19332584738731384\n",
      "epoch: 1 step: 321, loss is 0.3150571584701538\n",
      "epoch: 1 step: 322, loss is 0.19905541837215424\n",
      "epoch: 1 step: 323, loss is 0.26757869124412537\n",
      "epoch: 1 step: 324, loss is 0.2193644642829895\n",
      "epoch: 1 step: 325, loss is 0.4122121334075928\n",
      "epoch: 1 step: 326, loss is 0.25629064440727234\n",
      "epoch: 1 step: 327, loss is 0.3018938899040222\n",
      "epoch: 1 step: 328, loss is 0.416476845741272\n",
      "epoch: 1 step: 329, loss is 0.2214101403951645\n",
      "epoch: 1 step: 330, loss is 0.32718798518180847\n",
      "epoch: 1 step: 331, loss is 0.35682374238967896\n",
      "epoch: 1 step: 332, loss is 0.1941339075565338\n",
      "epoch: 1 step: 333, loss is 0.28409823775291443\n",
      "epoch: 1 step: 334, loss is 0.22444261610507965\n",
      "epoch: 1 step: 335, loss is 0.3321850001811981\n",
      "epoch: 1 step: 336, loss is 0.24645040929317474\n",
      "epoch: 1 step: 337, loss is 0.2536967098712921\n",
      "epoch: 1 step: 338, loss is 0.3096991181373596\n",
      "epoch: 1 step: 339, loss is 0.2944462299346924\n",
      "epoch: 1 step: 340, loss is 0.3704063296318054\n",
      "epoch: 1 step: 341, loss is 0.2366504669189453\n",
      "epoch: 1 step: 342, loss is 0.24050165712833405\n",
      "epoch: 1 step: 343, loss is 0.23997415602207184\n",
      "epoch: 1 step: 344, loss is 0.3417966961860657\n",
      "epoch: 1 step: 345, loss is 0.3076140284538269\n",
      "epoch: 1 step: 346, loss is 0.18685153126716614\n",
      "epoch: 1 step: 347, loss is 0.2657493054866791\n",
      "epoch: 1 step: 348, loss is 0.2587965130805969\n",
      "epoch: 1 step: 349, loss is 0.3084622621536255\n",
      "epoch: 1 step: 350, loss is 0.1713339388370514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 351, loss is 0.24092860519886017\n",
      "epoch: 1 step: 352, loss is 0.2609979212284088\n",
      "epoch: 1 step: 353, loss is 0.22612127661705017\n",
      "epoch: 1 step: 354, loss is 0.15764354169368744\n",
      "epoch: 1 step: 355, loss is 0.2490338385105133\n",
      "epoch: 1 step: 356, loss is 0.1737622320652008\n",
      "epoch: 1 step: 357, loss is 0.19827839732170105\n",
      "epoch: 1 step: 358, loss is 0.2095729410648346\n",
      "epoch: 1 step: 359, loss is 0.16658316552639008\n",
      "epoch: 1 step: 360, loss is 0.22187964618206024\n",
      "epoch: 1 step: 361, loss is 0.1519053429365158\n",
      "epoch: 1 step: 362, loss is 0.3275908827781677\n",
      "epoch: 1 step: 363, loss is 0.2237405627965927\n",
      "epoch: 1 step: 364, loss is 0.24688521027565002\n",
      "epoch: 1 step: 365, loss is 0.20651814341545105\n",
      "epoch: 1 step: 366, loss is 0.2604478895664215\n",
      "epoch: 1 step: 367, loss is 0.22888705134391785\n",
      "epoch: 1 step: 368, loss is 0.28472900390625\n",
      "epoch: 1 step: 369, loss is 0.19438999891281128\n",
      "epoch: 1 step: 370, loss is 0.3789155185222626\n",
      "epoch: 1 step: 371, loss is 0.17261230945587158\n",
      "epoch: 1 step: 372, loss is 0.23324471712112427\n",
      "epoch: 1 step: 373, loss is 0.2551840543746948\n",
      "epoch: 1 step: 374, loss is 0.15792857110500336\n",
      "epoch: 1 step: 375, loss is 0.23297511041164398\n",
      "epoch: 1 step: 376, loss is 0.1953934133052826\n",
      "epoch: 1 step: 377, loss is 0.1952001303434372\n",
      "epoch: 1 step: 378, loss is 0.2764616012573242\n",
      "epoch: 1 step: 379, loss is 0.23107390105724335\n",
      "epoch: 1 step: 380, loss is 0.09213176369667053\n",
      "epoch: 1 step: 381, loss is 0.17805561423301697\n",
      "epoch: 1 step: 382, loss is 0.2677121162414551\n",
      "epoch: 1 step: 383, loss is 0.23349745571613312\n",
      "epoch: 1 step: 384, loss is 0.18589970469474792\n",
      "epoch: 1 step: 385, loss is 0.17948338389396667\n",
      "epoch: 1 step: 386, loss is 0.128671333193779\n",
      "epoch: 1 step: 387, loss is 0.1904822289943695\n",
      "epoch: 1 step: 388, loss is 0.28467464447021484\n",
      "epoch: 1 step: 389, loss is 0.22186967730522156\n",
      "epoch: 1 step: 390, loss is 0.14729711413383484\n",
      "epoch: 1 step: 391, loss is 0.15710987150669098\n",
      "epoch: 1 step: 392, loss is 0.1828116476535797\n",
      "epoch: 1 step: 393, loss is 0.2260216623544693\n",
      "epoch: 1 step: 394, loss is 0.26342421770095825\n",
      "epoch: 1 step: 395, loss is 0.21957029402256012\n",
      "epoch: 1 step: 396, loss is 0.1634298413991928\n",
      "epoch: 1 step: 397, loss is 0.15072931349277496\n",
      "epoch: 1 step: 398, loss is 0.20002005994319916\n",
      "epoch: 1 step: 399, loss is 0.23989614844322205\n",
      "epoch: 1 step: 400, loss is 0.19424937665462494\n",
      "epoch: 1 step: 401, loss is 0.17962035536766052\n",
      "epoch: 1 step: 402, loss is 0.1354386955499649\n",
      "epoch: 1 step: 403, loss is 0.24081864953041077\n",
      "epoch: 1 step: 404, loss is 0.08049924671649933\n",
      "epoch: 1 step: 405, loss is 0.12573233246803284\n",
      "epoch: 1 step: 406, loss is 0.19538390636444092\n",
      "epoch: 1 step: 407, loss is 0.0961245745420456\n",
      "epoch: 1 step: 408, loss is 0.20346905291080475\n",
      "epoch: 1 step: 409, loss is 0.1707611083984375\n",
      "epoch: 1 step: 410, loss is 0.08804070949554443\n",
      "epoch: 1 step: 411, loss is 0.21468794345855713\n",
      "epoch: 1 step: 412, loss is 0.1599036157131195\n",
      "epoch: 1 step: 413, loss is 0.10455736517906189\n",
      "epoch: 1 step: 414, loss is 0.10078375041484833\n",
      "epoch: 1 step: 415, loss is 0.16927337646484375\n",
      "epoch: 1 step: 416, loss is 0.17971011996269226\n",
      "epoch: 1 step: 417, loss is 0.2225261926651001\n",
      "epoch: 1 step: 418, loss is 0.1994895488023758\n",
      "epoch: 1 step: 419, loss is 0.0893731564283371\n",
      "epoch: 1 step: 420, loss is 0.140917107462883\n",
      "epoch: 1 step: 421, loss is 0.14282362163066864\n",
      "epoch: 1 step: 422, loss is 0.11372285336256027\n",
      "epoch: 1 step: 423, loss is 0.17082567512989044\n",
      "epoch: 1 step: 424, loss is 0.19374454021453857\n",
      "epoch: 1 step: 425, loss is 0.18918268382549286\n",
      "epoch: 1 step: 426, loss is 0.20871871709823608\n",
      "epoch: 1 step: 427, loss is 0.11079703271389008\n",
      "epoch: 1 step: 428, loss is 0.1654590368270874\n",
      "epoch: 1 step: 429, loss is 0.09468238055706024\n",
      "epoch: 1 step: 430, loss is 0.19901412725448608\n",
      "epoch: 1 step: 431, loss is 0.12106144428253174\n",
      "epoch: 1 step: 432, loss is 0.14274324476718903\n",
      "epoch: 1 step: 433, loss is 0.09436078369617462\n",
      "epoch: 1 step: 434, loss is 0.0777173861861229\n",
      "epoch: 1 step: 435, loss is 0.18677352368831635\n",
      "epoch: 1 step: 436, loss is 0.1442825198173523\n",
      "epoch: 1 step: 437, loss is 0.10186842828989029\n",
      "epoch: 1 step: 438, loss is 0.1512397974729538\n",
      "epoch: 1 step: 439, loss is 0.23817268013954163\n",
      "epoch: 1 step: 440, loss is 0.08177026361227036\n",
      "epoch: 1 step: 441, loss is 0.11340003460645676\n",
      "epoch: 1 step: 442, loss is 0.13208690285682678\n",
      "epoch: 1 step: 443, loss is 0.110871821641922\n",
      "epoch: 1 step: 444, loss is 0.22021234035491943\n",
      "epoch: 1 step: 445, loss is 0.25939053297042847\n",
      "epoch: 1 step: 446, loss is 0.0803525447845459\n",
      "epoch: 1 step: 447, loss is 0.0683506578207016\n",
      "epoch: 1 step: 448, loss is 0.1290006935596466\n",
      "epoch: 1 step: 449, loss is 0.13344305753707886\n",
      "epoch: 1 step: 450, loss is 0.1499437540769577\n",
      "epoch: 1 step: 451, loss is 0.1612507402896881\n",
      "epoch: 1 step: 452, loss is 0.1293049156665802\n",
      "epoch: 1 step: 453, loss is 0.11925676465034485\n",
      "epoch: 1 step: 454, loss is 0.15649613738059998\n",
      "epoch: 1 step: 455, loss is 0.08091019839048386\n",
      "epoch: 1 step: 456, loss is 0.13869565725326538\n",
      "epoch: 1 step: 457, loss is 0.21740464866161346\n",
      "epoch: 1 step: 458, loss is 0.14839157462120056\n",
      "epoch: 1 step: 459, loss is 0.08848297595977783\n",
      "epoch: 1 step: 460, loss is 0.2094508409500122\n",
      "epoch: 1 step: 461, loss is 0.1264292299747467\n",
      "epoch: 1 step: 462, loss is 0.1494550108909607\n",
      "epoch: 1 step: 463, loss is 0.08222509920597076\n",
      "epoch: 1 step: 464, loss is 0.14455509185791016\n",
      "epoch: 1 step: 465, loss is 0.1792527735233307\n",
      "epoch: 1 step: 466, loss is 0.10778236389160156\n",
      "epoch: 1 step: 467, loss is 0.08134099096059799\n",
      "epoch: 1 step: 468, loss is 0.0976393073797226\n",
      "epoch: 1 step: 469, loss is 0.05229680612683296\n",
      "epoch: 1 step: 470, loss is 0.17184200882911682\n",
      "epoch: 1 step: 471, loss is 0.09390987455844879\n",
      "epoch: 1 step: 472, loss is 0.09230833500623703\n",
      "epoch: 1 step: 473, loss is 0.1173187792301178\n",
      "epoch: 1 step: 474, loss is 0.23167409002780914\n",
      "epoch: 1 step: 475, loss is 0.13540518283843994\n",
      "epoch: 1 step: 476, loss is 0.17074982821941376\n",
      "epoch: 1 step: 477, loss is 0.27729532122612\n",
      "epoch: 1 step: 478, loss is 0.0855809897184372\n",
      "epoch: 1 step: 479, loss is 0.12871360778808594\n",
      "epoch: 1 step: 480, loss is 0.24333833158016205\n",
      "epoch: 1 step: 481, loss is 0.07405953109264374\n",
      "epoch: 1 step: 482, loss is 0.07076326012611389\n",
      "epoch: 1 step: 483, loss is 0.13867273926734924\n",
      "epoch: 1 step: 484, loss is 0.12777429819107056\n",
      "epoch: 1 step: 485, loss is 0.0835917592048645\n",
      "epoch: 1 step: 486, loss is 0.10710164904594421\n",
      "epoch: 1 step: 487, loss is 0.16458368301391602\n",
      "epoch: 1 step: 488, loss is 0.11652871966362\n",
      "epoch: 1 step: 489, loss is 0.20897425711154938\n",
      "epoch: 1 step: 490, loss is 0.08443853259086609\n",
      "epoch: 1 step: 491, loss is 0.12041649967432022\n",
      "epoch: 1 step: 492, loss is 0.1533435881137848\n",
      "epoch: 1 step: 493, loss is 0.1189441978931427\n",
      "epoch: 1 step: 494, loss is 0.19789792597293854\n",
      "epoch: 1 step: 495, loss is 0.05550864338874817\n",
      "epoch: 1 step: 496, loss is 0.12951910495758057\n",
      "epoch: 1 step: 497, loss is 0.14364972710609436\n",
      "epoch: 1 step: 498, loss is 0.11338850110769272\n",
      "epoch: 1 step: 499, loss is 0.04784459248185158\n",
      "epoch: 1 step: 500, loss is 0.11722074449062347\n",
      "epoch: 1 step: 501, loss is 0.112697072327137\n",
      "epoch: 1 step: 502, loss is 0.10718891024589539\n",
      "epoch: 1 step: 503, loss is 0.056040458381175995\n",
      "epoch: 1 step: 504, loss is 0.08026491850614548\n",
      "epoch: 1 step: 505, loss is 0.05426694452762604\n",
      "epoch: 1 step: 506, loss is 0.08257590234279633\n",
      "epoch: 1 step: 507, loss is 0.1286068856716156\n",
      "epoch: 1 step: 508, loss is 0.06125681847333908\n",
      "epoch: 1 step: 509, loss is 0.058038271963596344\n",
      "epoch: 1 step: 510, loss is 0.028786201030015945\n",
      "epoch: 1 step: 511, loss is 0.15576735138893127\n",
      "epoch: 1 step: 512, loss is 0.14895324409008026\n",
      "epoch: 1 step: 513, loss is 0.0968230739235878\n",
      "epoch: 1 step: 514, loss is 0.09928467869758606\n",
      "epoch: 1 step: 515, loss is 0.10397348552942276\n",
      "epoch: 1 step: 516, loss is 0.1487543284893036\n",
      "epoch: 1 step: 517, loss is 0.1170487329363823\n",
      "epoch: 1 step: 518, loss is 0.07313423603773117\n",
      "epoch: 1 step: 519, loss is 0.19986195862293243\n",
      "epoch: 1 step: 520, loss is 0.055550917983055115\n",
      "epoch: 1 step: 521, loss is 0.09245249629020691\n",
      "epoch: 1 step: 522, loss is 0.12365269660949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 523, loss is 0.036893170326948166\n",
      "epoch: 1 step: 524, loss is 0.1346534639596939\n",
      "epoch: 1 step: 525, loss is 0.07671492546796799\n",
      "epoch: 1 step: 526, loss is 0.07687612622976303\n",
      "epoch: 1 step: 527, loss is 0.12714266777038574\n",
      "epoch: 1 step: 528, loss is 0.11988267302513123\n",
      "epoch: 1 step: 529, loss is 0.05317528545856476\n",
      "epoch: 1 step: 530, loss is 0.07822000980377197\n",
      "epoch: 1 step: 531, loss is 0.07220830768346786\n",
      "epoch: 1 step: 532, loss is 0.09454888105392456\n",
      "epoch: 1 step: 533, loss is 0.07116413116455078\n",
      "epoch: 1 step: 534, loss is 0.09042485058307648\n",
      "epoch: 1 step: 535, loss is 0.05846816673874855\n",
      "epoch: 1 step: 536, loss is 0.08718211203813553\n",
      "epoch: 1 step: 537, loss is 0.0961541011929512\n",
      "epoch: 1 step: 538, loss is 0.0866866260766983\n",
      "epoch: 1 step: 539, loss is 0.031959906220436096\n",
      "epoch: 1 step: 540, loss is 0.049598097801208496\n",
      "epoch: 1 step: 541, loss is 0.0938214659690857\n",
      "epoch: 1 step: 542, loss is 0.13128870725631714\n",
      "epoch: 1 step: 543, loss is 0.10612668842077255\n",
      "epoch: 1 step: 544, loss is 0.08971361070871353\n",
      "epoch: 1 step: 545, loss is 0.051521800458431244\n",
      "epoch: 1 step: 546, loss is 0.07381930202245712\n",
      "epoch: 1 step: 547, loss is 0.04764025658369064\n",
      "epoch: 1 step: 548, loss is 0.15074211359024048\n",
      "epoch: 1 step: 549, loss is 0.12024404853582382\n",
      "epoch: 1 step: 550, loss is 0.07667529582977295\n",
      "epoch: 1 step: 551, loss is 0.05766547843813896\n",
      "epoch: 1 step: 552, loss is 0.12254532426595688\n",
      "epoch: 1 step: 553, loss is 0.059992097318172455\n",
      "epoch: 1 step: 554, loss is 0.0619320347905159\n",
      "epoch: 1 step: 555, loss is 0.0628127008676529\n",
      "epoch: 1 step: 556, loss is 0.03257203847169876\n",
      "epoch: 1 step: 557, loss is 0.08023776859045029\n",
      "epoch: 1 step: 558, loss is 0.0873299092054367\n",
      "epoch: 1 step: 559, loss is 0.03949592262506485\n",
      "epoch: 1 step: 560, loss is 0.054851386696100235\n",
      "epoch: 1 step: 561, loss is 0.06351548433303833\n",
      "epoch: 1 step: 562, loss is 0.02937048301100731\n",
      "epoch: 1 step: 563, loss is 0.03951990604400635\n",
      "epoch: 1 step: 564, loss is 0.033721938729286194\n",
      "epoch: 1 step: 565, loss is 0.12346026301383972\n",
      "epoch: 1 step: 566, loss is 0.07439563423395157\n",
      "epoch: 1 step: 567, loss is 0.04715443402528763\n",
      "epoch: 1 step: 568, loss is 0.040781572461128235\n",
      "epoch: 1 step: 569, loss is 0.09007624536752701\n",
      "epoch: 1 step: 570, loss is 0.05941130965948105\n",
      "epoch: 1 step: 571, loss is 0.1001223474740982\n",
      "epoch: 1 step: 572, loss is 0.1002543494105339\n",
      "epoch: 1 step: 573, loss is 0.09110614657402039\n",
      "epoch: 1 step: 574, loss is 0.09582225978374481\n",
      "epoch: 1 step: 575, loss is 0.07006072998046875\n",
      "epoch: 1 step: 576, loss is 0.03465757519006729\n",
      "epoch: 1 step: 577, loss is 0.11890912801027298\n",
      "epoch: 1 step: 578, loss is 0.028796032071113586\n",
      "epoch: 1 step: 579, loss is 0.12486101686954498\n",
      "epoch: 1 step: 580, loss is 0.031084973365068436\n",
      "epoch: 1 step: 581, loss is 0.06583695113658905\n",
      "epoch: 1 step: 582, loss is 0.041235413402318954\n",
      "epoch: 1 step: 583, loss is 0.04073382914066315\n",
      "epoch: 1 step: 584, loss is 0.13246482610702515\n",
      "epoch: 1 step: 585, loss is 0.03661496192216873\n",
      "epoch: 1 step: 586, loss is 0.030494045466184616\n",
      "epoch: 1 step: 587, loss is 0.05854182317852974\n",
      "epoch: 1 step: 588, loss is 0.10416177660226822\n",
      "epoch: 1 step: 589, loss is 0.05137531831860542\n",
      "epoch: 1 step: 590, loss is 0.0977945625782013\n",
      "epoch: 1 step: 591, loss is 0.047127675265073776\n",
      "epoch: 1 step: 592, loss is 0.02689656987786293\n",
      "epoch: 1 step: 593, loss is 0.13526859879493713\n",
      "epoch: 1 step: 594, loss is 0.15698713064193726\n",
      "epoch: 1 step: 595, loss is 0.023123184219002724\n",
      "epoch: 1 step: 596, loss is 0.031676698476076126\n",
      "Train epoch time: 30032.571 ms, per step time: 50.390 ms\n",
      "epoch: 2 step: 1, loss is 0.08402729034423828\n",
      "epoch: 2 step: 2, loss is 0.04751937836408615\n",
      "epoch: 2 step: 3, loss is 0.06425497680902481\n",
      "epoch: 2 step: 4, loss is 0.08965892344713211\n",
      "epoch: 2 step: 5, loss is 0.0716613382101059\n",
      "epoch: 2 step: 6, loss is 0.046007826924324036\n",
      "epoch: 2 step: 7, loss is 0.039664000272750854\n",
      "epoch: 2 step: 8, loss is 0.02042064629495144\n",
      "epoch: 2 step: 9, loss is 0.05224619060754776\n",
      "epoch: 2 step: 10, loss is 0.16408120095729828\n",
      "epoch: 2 step: 11, loss is 0.05333542078733444\n",
      "epoch: 2 step: 12, loss is 0.038858432322740555\n",
      "epoch: 2 step: 13, loss is 0.10936717689037323\n",
      "epoch: 2 step: 14, loss is 0.06671856343746185\n",
      "epoch: 2 step: 15, loss is 0.061590902507305145\n",
      "epoch: 2 step: 16, loss is 0.04557711258530617\n",
      "epoch: 2 step: 17, loss is 0.06222423166036606\n",
      "epoch: 2 step: 18, loss is 0.11429950594902039\n",
      "epoch: 2 step: 19, loss is 0.04190739989280701\n",
      "epoch: 2 step: 20, loss is 0.03602258861064911\n",
      "epoch: 2 step: 21, loss is 0.030252553522586823\n",
      "epoch: 2 step: 22, loss is 0.029220502823591232\n",
      "epoch: 2 step: 23, loss is 0.04146469384431839\n",
      "epoch: 2 step: 24, loss is 0.021916916593909264\n",
      "epoch: 2 step: 25, loss is 0.039388515055179596\n",
      "epoch: 2 step: 26, loss is 0.033460088074207306\n",
      "epoch: 2 step: 27, loss is 0.13810452818870544\n",
      "epoch: 2 step: 28, loss is 0.044647689908742905\n",
      "epoch: 2 step: 29, loss is 0.07068494707345963\n",
      "epoch: 2 step: 30, loss is 0.21355290710926056\n",
      "epoch: 2 step: 31, loss is 0.04213649034500122\n",
      "epoch: 2 step: 32, loss is 0.030898109078407288\n",
      "epoch: 2 step: 33, loss is 0.13345888257026672\n",
      "epoch: 2 step: 34, loss is 0.019383909180760384\n",
      "epoch: 2 step: 35, loss is 0.030829373747110367\n",
      "epoch: 2 step: 36, loss is 0.04448632895946503\n",
      "epoch: 2 step: 37, loss is 0.0413680225610733\n",
      "epoch: 2 step: 38, loss is 0.027579374611377716\n",
      "epoch: 2 step: 39, loss is 0.07570400834083557\n",
      "epoch: 2 step: 40, loss is 0.0690026730298996\n",
      "epoch: 2 step: 41, loss is 0.09810934960842133\n",
      "epoch: 2 step: 42, loss is 0.11779758334159851\n",
      "epoch: 2 step: 43, loss is 0.04235607385635376\n",
      "epoch: 2 step: 44, loss is 0.06915062665939331\n",
      "epoch: 2 step: 45, loss is 0.0827193409204483\n",
      "epoch: 2 step: 46, loss is 0.03725382685661316\n",
      "epoch: 2 step: 47, loss is 0.06443536281585693\n",
      "epoch: 2 step: 48, loss is 0.028616737574338913\n",
      "epoch: 2 step: 49, loss is 0.027796979993581772\n",
      "epoch: 2 step: 50, loss is 0.04646969214081764\n",
      "epoch: 2 step: 51, loss is 0.054140739142894745\n",
      "epoch: 2 step: 52, loss is 0.015474619343876839\n",
      "epoch: 2 step: 53, loss is 0.025707541033625603\n",
      "epoch: 2 step: 54, loss is 0.03994660824537277\n",
      "epoch: 2 step: 55, loss is 0.07572580873966217\n",
      "epoch: 2 step: 56, loss is 0.026924610137939453\n",
      "epoch: 2 step: 57, loss is 0.010961008258163929\n",
      "epoch: 2 step: 58, loss is 0.013177169486880302\n",
      "epoch: 2 step: 59, loss is 0.02897173911333084\n",
      "epoch: 2 step: 60, loss is 0.02549479342997074\n",
      "epoch: 2 step: 61, loss is 0.040226370096206665\n",
      "epoch: 2 step: 62, loss is 0.018793731927871704\n",
      "epoch: 2 step: 63, loss is 0.01304804440587759\n",
      "epoch: 2 step: 64, loss is 0.07339933514595032\n",
      "epoch: 2 step: 65, loss is 0.06155695766210556\n",
      "epoch: 2 step: 66, loss is 0.060619428753852844\n",
      "epoch: 2 step: 67, loss is 0.043585777282714844\n",
      "epoch: 2 step: 68, loss is 0.0918324664235115\n",
      "epoch: 2 step: 69, loss is 0.09602560102939606\n",
      "epoch: 2 step: 70, loss is 0.048761676996946335\n",
      "epoch: 2 step: 71, loss is 0.015173029154539108\n",
      "epoch: 2 step: 72, loss is 0.08836376667022705\n",
      "epoch: 2 step: 73, loss is 0.014519905671477318\n",
      "epoch: 2 step: 74, loss is 0.054117538034915924\n",
      "epoch: 2 step: 75, loss is 0.06106530502438545\n",
      "epoch: 2 step: 76, loss is 0.010485215112566948\n",
      "epoch: 2 step: 77, loss is 0.06964603066444397\n",
      "epoch: 2 step: 78, loss is 0.019910180941224098\n",
      "epoch: 2 step: 79, loss is 0.07220689207315445\n",
      "epoch: 2 step: 80, loss is 0.05196885019540787\n",
      "epoch: 2 step: 81, loss is 0.04111400991678238\n",
      "epoch: 2 step: 82, loss is 0.019947784021496773\n",
      "epoch: 2 step: 83, loss is 0.03856591135263443\n",
      "epoch: 2 step: 84, loss is 0.06642129272222519\n",
      "epoch: 2 step: 85, loss is 0.020357415080070496\n",
      "epoch: 2 step: 86, loss is 0.02918126806616783\n",
      "epoch: 2 step: 87, loss is 0.05321331322193146\n",
      "epoch: 2 step: 88, loss is 0.01656562276184559\n",
      "epoch: 2 step: 89, loss is 0.01883964240550995\n",
      "epoch: 2 step: 90, loss is 0.04197930544614792\n",
      "epoch: 2 step: 91, loss is 0.04459875822067261\n",
      "epoch: 2 step: 92, loss is 0.02905425801873207\n",
      "epoch: 2 step: 93, loss is 0.02642286755144596\n",
      "epoch: 2 step: 94, loss is 0.02930588833987713\n",
      "epoch: 2 step: 95, loss is 0.059244927018880844\n",
      "epoch: 2 step: 96, loss is 0.026676731184124947\n",
      "epoch: 2 step: 97, loss is 0.009552022442221642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 98, loss is 0.026237521320581436\n",
      "epoch: 2 step: 99, loss is 0.04399152472615242\n",
      "epoch: 2 step: 100, loss is 0.04403179511427879\n",
      "epoch: 2 step: 101, loss is 0.07010473310947418\n",
      "epoch: 2 step: 102, loss is 0.02898583747446537\n",
      "epoch: 2 step: 103, loss is 0.026590269058942795\n",
      "epoch: 2 step: 104, loss is 0.02109355479478836\n",
      "epoch: 2 step: 105, loss is 0.07162576168775558\n",
      "epoch: 2 step: 106, loss is 0.03113730251789093\n",
      "epoch: 2 step: 107, loss is 0.029665354639291763\n",
      "epoch: 2 step: 108, loss is 0.025386393070220947\n",
      "epoch: 2 step: 109, loss is 0.04268551617860794\n",
      "epoch: 2 step: 110, loss is 0.038048576563596725\n",
      "epoch: 2 step: 111, loss is 0.08531243354082108\n",
      "epoch: 2 step: 112, loss is 0.011992910876870155\n",
      "epoch: 2 step: 113, loss is 0.023704057559370995\n",
      "epoch: 2 step: 114, loss is 0.024038149043917656\n",
      "epoch: 2 step: 115, loss is 0.006830915808677673\n",
      "epoch: 2 step: 116, loss is 0.014070697128772736\n",
      "epoch: 2 step: 117, loss is 0.020277049392461777\n",
      "epoch: 2 step: 118, loss is 0.056528475135564804\n",
      "epoch: 2 step: 119, loss is 0.05074344202876091\n",
      "epoch: 2 step: 120, loss is 0.014078272506594658\n",
      "epoch: 2 step: 121, loss is 0.006525490432977676\n",
      "epoch: 2 step: 122, loss is 0.028072498738765717\n",
      "epoch: 2 step: 123, loss is 0.007797976024448872\n",
      "epoch: 2 step: 124, loss is 0.04134043678641319\n",
      "epoch: 2 step: 125, loss is 0.015176592394709587\n",
      "epoch: 2 step: 126, loss is 0.015366102568805218\n",
      "epoch: 2 step: 127, loss is 0.013792017474770546\n",
      "epoch: 2 step: 128, loss is 0.025284195318818092\n",
      "epoch: 2 step: 129, loss is 0.025425467640161514\n",
      "epoch: 2 step: 130, loss is 0.018060889095067978\n",
      "epoch: 2 step: 131, loss is 0.008967566303908825\n",
      "epoch: 2 step: 132, loss is 0.026789113879203796\n",
      "epoch: 2 step: 133, loss is 0.026562565937638283\n",
      "epoch: 2 step: 134, loss is 0.022741619497537613\n",
      "epoch: 2 step: 135, loss is 0.02097673900425434\n",
      "epoch: 2 step: 136, loss is 0.026614023372530937\n",
      "epoch: 2 step: 137, loss is 0.05563449114561081\n",
      "epoch: 2 step: 138, loss is 0.015392059460282326\n",
      "epoch: 2 step: 139, loss is 0.017465945333242416\n",
      "epoch: 2 step: 140, loss is 0.02193181961774826\n",
      "epoch: 2 step: 141, loss is 0.03537263348698616\n",
      "epoch: 2 step: 142, loss is 0.01514817588031292\n",
      "epoch: 2 step: 143, loss is 0.06369628757238388\n",
      "epoch: 2 step: 144, loss is 0.016236940398812294\n",
      "epoch: 2 step: 145, loss is 0.008225546218454838\n",
      "epoch: 2 step: 146, loss is 0.08827365934848785\n",
      "epoch: 2 step: 147, loss is 0.10017697513103485\n",
      "epoch: 2 step: 148, loss is 0.011081894859671593\n",
      "epoch: 2 step: 149, loss is 0.013117220252752304\n",
      "epoch: 2 step: 150, loss is 0.037079017609357834\n",
      "epoch: 2 step: 151, loss is 0.010235168039798737\n",
      "epoch: 2 step: 152, loss is 0.018393786624073982\n",
      "epoch: 2 step: 153, loss is 0.039489977061748505\n",
      "epoch: 2 step: 154, loss is 0.019540540874004364\n",
      "epoch: 2 step: 155, loss is 0.019833490252494812\n",
      "epoch: 2 step: 156, loss is 0.024146128445863724\n",
      "epoch: 2 step: 157, loss is 0.00770612433552742\n",
      "epoch: 2 step: 158, loss is 0.0395023450255394\n",
      "epoch: 2 step: 159, loss is 0.06324509531259537\n",
      "epoch: 2 step: 160, loss is 0.021411573514342308\n",
      "epoch: 2 step: 161, loss is 0.0223541222512722\n",
      "epoch: 2 step: 162, loss is 0.040030285716056824\n",
      "epoch: 2 step: 163, loss is 0.00900074653327465\n",
      "epoch: 2 step: 164, loss is 0.057840265333652496\n",
      "epoch: 2 step: 165, loss is 0.0239276010543108\n",
      "epoch: 2 step: 166, loss is 0.018317988142371178\n",
      "epoch: 2 step: 167, loss is 0.040332913398742676\n",
      "epoch: 2 step: 168, loss is 0.023116011172533035\n",
      "epoch: 2 step: 169, loss is 0.046196725219488144\n",
      "epoch: 2 step: 170, loss is 0.007047395221889019\n",
      "epoch: 2 step: 171, loss is 0.009587973356246948\n",
      "epoch: 2 step: 172, loss is 0.01914580538868904\n",
      "epoch: 2 step: 173, loss is 0.007165803574025631\n",
      "epoch: 2 step: 174, loss is 0.025144631043076515\n",
      "epoch: 2 step: 175, loss is 0.007402487099170685\n",
      "epoch: 2 step: 176, loss is 0.05678614601492882\n",
      "epoch: 2 step: 177, loss is 0.011620962992310524\n",
      "epoch: 2 step: 178, loss is 0.022581804543733597\n",
      "epoch: 2 step: 179, loss is 0.07112731784582138\n",
      "epoch: 2 step: 180, loss is 0.00899549014866352\n",
      "epoch: 2 step: 181, loss is 0.021021582186222076\n",
      "epoch: 2 step: 182, loss is 0.08102639764547348\n",
      "epoch: 2 step: 183, loss is 0.011860025115311146\n",
      "epoch: 2 step: 184, loss is 0.016713842749595642\n",
      "epoch: 2 step: 185, loss is 0.02230963110923767\n",
      "epoch: 2 step: 186, loss is 0.032920897006988525\n",
      "epoch: 2 step: 187, loss is 0.012745393440127373\n",
      "epoch: 2 step: 188, loss is 0.0516858734190464\n",
      "epoch: 2 step: 189, loss is 0.008032066747546196\n",
      "epoch: 2 step: 190, loss is 0.01857461780309677\n",
      "epoch: 2 step: 191, loss is 0.03216099739074707\n",
      "epoch: 2 step: 192, loss is 0.014349455013871193\n",
      "epoch: 2 step: 193, loss is 0.02151053585112095\n",
      "epoch: 2 step: 194, loss is 0.04342217743396759\n",
      "epoch: 2 step: 195, loss is 0.008837204426527023\n",
      "epoch: 2 step: 196, loss is 0.040236227214336395\n",
      "epoch: 2 step: 197, loss is 0.03158935531973839\n",
      "epoch: 2 step: 198, loss is 0.014835409820079803\n",
      "epoch: 2 step: 199, loss is 0.02026519551873207\n",
      "epoch: 2 step: 200, loss is 0.013799477368593216\n",
      "epoch: 2 step: 201, loss is 0.01973748952150345\n",
      "epoch: 2 step: 202, loss is 0.02664719894528389\n",
      "epoch: 2 step: 203, loss is 0.016963928937911987\n",
      "epoch: 2 step: 204, loss is 0.030152196064591408\n",
      "epoch: 2 step: 205, loss is 0.007713582366704941\n",
      "epoch: 2 step: 206, loss is 0.026240123435854912\n",
      "epoch: 2 step: 207, loss is 0.004758027382194996\n",
      "epoch: 2 step: 208, loss is 0.01011087279766798\n",
      "epoch: 2 step: 209, loss is 0.01358798798173666\n",
      "epoch: 2 step: 210, loss is 0.008284475654363632\n",
      "epoch: 2 step: 211, loss is 0.00649065850302577\n",
      "epoch: 2 step: 212, loss is 0.006574451923370361\n",
      "epoch: 2 step: 213, loss is 0.03354128822684288\n",
      "epoch: 2 step: 214, loss is 0.018918007612228394\n",
      "epoch: 2 step: 215, loss is 0.023802563548088074\n",
      "epoch: 2 step: 216, loss is 0.030602071434259415\n",
      "epoch: 2 step: 217, loss is 0.014703481458127499\n",
      "epoch: 2 step: 218, loss is 0.03444519266486168\n",
      "epoch: 2 step: 219, loss is 0.015202481299638748\n",
      "epoch: 2 step: 220, loss is 0.009429104626178741\n",
      "epoch: 2 step: 221, loss is 0.05332513526082039\n",
      "epoch: 2 step: 222, loss is 0.005143688526004553\n",
      "epoch: 2 step: 223, loss is 0.03612556308507919\n",
      "epoch: 2 step: 224, loss is 0.012763749808073044\n",
      "epoch: 2 step: 225, loss is 0.005436989478766918\n",
      "epoch: 2 step: 226, loss is 0.01570737175643444\n",
      "epoch: 2 step: 227, loss is 0.009099354967474937\n",
      "epoch: 2 step: 228, loss is 0.016452107578516006\n",
      "epoch: 2 step: 229, loss is 0.007350421044975519\n",
      "epoch: 2 step: 230, loss is 0.02733001485466957\n",
      "epoch: 2 step: 231, loss is 0.0069162556901574135\n",
      "epoch: 2 step: 232, loss is 0.012750886380672455\n",
      "epoch: 2 step: 233, loss is 0.013630373403429985\n",
      "epoch: 2 step: 234, loss is 0.010387256741523743\n",
      "epoch: 2 step: 235, loss is 0.015800841152668\n",
      "epoch: 2 step: 236, loss is 0.017542706802487373\n",
      "epoch: 2 step: 237, loss is 0.006298828404396772\n",
      "epoch: 2 step: 238, loss is 0.04268438741564751\n",
      "epoch: 2 step: 239, loss is 0.012815582565963268\n",
      "epoch: 2 step: 240, loss is 0.027022529393434525\n",
      "epoch: 2 step: 241, loss is 0.014986637979745865\n",
      "epoch: 2 step: 242, loss is 0.008443047292530537\n",
      "epoch: 2 step: 243, loss is 0.04088223725557327\n",
      "epoch: 2 step: 244, loss is 0.01396328117698431\n",
      "epoch: 2 step: 245, loss is 0.005718323402106762\n",
      "epoch: 2 step: 246, loss is 0.005297086201608181\n",
      "epoch: 2 step: 247, loss is 0.006899877917021513\n",
      "epoch: 2 step: 248, loss is 0.013353143818676472\n",
      "epoch: 2 step: 249, loss is 0.009472602047026157\n",
      "epoch: 2 step: 250, loss is 0.017732758074998856\n",
      "epoch: 2 step: 251, loss is 0.01513564120978117\n",
      "epoch: 2 step: 252, loss is 0.008251668885350227\n",
      "epoch: 2 step: 253, loss is 0.008965304121375084\n",
      "epoch: 2 step: 254, loss is 0.009621416218578815\n",
      "epoch: 2 step: 255, loss is 0.0190118420869112\n",
      "epoch: 2 step: 256, loss is 0.015494895167648792\n",
      "epoch: 2 step: 257, loss is 0.014616181142628193\n",
      "epoch: 2 step: 258, loss is 0.011320000514388084\n",
      "epoch: 2 step: 259, loss is 0.007114279083907604\n",
      "epoch: 2 step: 260, loss is 0.052812643349170685\n",
      "epoch: 2 step: 261, loss is 0.009328940883278847\n",
      "epoch: 2 step: 262, loss is 0.004273978527635336\n",
      "epoch: 2 step: 263, loss is 0.0213735681027174\n",
      "epoch: 2 step: 264, loss is 0.006701692007482052\n",
      "epoch: 2 step: 265, loss is 0.008475292474031448\n",
      "epoch: 2 step: 266, loss is 0.0031532994471490383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 267, loss is 0.02413078397512436\n",
      "epoch: 2 step: 268, loss is 0.008266668766736984\n",
      "epoch: 2 step: 269, loss is 0.01968947798013687\n",
      "epoch: 2 step: 270, loss is 0.004359885584563017\n",
      "epoch: 2 step: 271, loss is 0.011067138984799385\n",
      "epoch: 2 step: 272, loss is 0.003340118331834674\n",
      "epoch: 2 step: 273, loss is 0.009768541902303696\n",
      "epoch: 2 step: 274, loss is 0.01201885286718607\n",
      "epoch: 2 step: 275, loss is 0.017018459737300873\n",
      "epoch: 2 step: 276, loss is 0.013010584749281406\n",
      "epoch: 2 step: 277, loss is 0.007726775482296944\n",
      "epoch: 2 step: 278, loss is 0.011000252328813076\n",
      "epoch: 2 step: 279, loss is 0.005872378125786781\n",
      "epoch: 2 step: 280, loss is 0.0070030055940151215\n",
      "epoch: 2 step: 281, loss is 0.016099099069833755\n",
      "epoch: 2 step: 282, loss is 0.0054572937078773975\n",
      "epoch: 2 step: 283, loss is 0.06694652140140533\n",
      "epoch: 2 step: 284, loss is 0.01008275430649519\n",
      "epoch: 2 step: 285, loss is 0.015508952550590038\n",
      "epoch: 2 step: 286, loss is 0.054250217974185944\n",
      "epoch: 2 step: 287, loss is 0.007133547216653824\n",
      "epoch: 2 step: 288, loss is 0.006857939530164003\n",
      "epoch: 2 step: 289, loss is 0.024237308651208878\n",
      "epoch: 2 step: 290, loss is 0.05248752981424332\n",
      "epoch: 2 step: 291, loss is 0.005786771420389414\n",
      "epoch: 2 step: 292, loss is 0.02671303227543831\n",
      "epoch: 2 step: 293, loss is 0.004441477824002504\n",
      "epoch: 2 step: 294, loss is 0.003053896129131317\n",
      "epoch: 2 step: 295, loss is 0.03823399543762207\n",
      "epoch: 2 step: 296, loss is 0.056081075221300125\n",
      "epoch: 2 step: 297, loss is 0.0052757831290364265\n",
      "epoch: 2 step: 298, loss is 0.0045576076954603195\n",
      "epoch: 2 step: 299, loss is 0.018072042614221573\n",
      "epoch: 2 step: 300, loss is 0.01061380933970213\n",
      "epoch: 2 step: 301, loss is 0.009654488414525986\n",
      "epoch: 2 step: 302, loss is 0.01379866898059845\n",
      "epoch: 2 step: 303, loss is 0.010625970549881458\n",
      "epoch: 2 step: 304, loss is 0.0073104482144117355\n",
      "epoch: 2 step: 305, loss is 0.007199096493422985\n",
      "epoch: 2 step: 306, loss is 0.004092431627213955\n",
      "epoch: 2 step: 307, loss is 0.005205444525927305\n",
      "epoch: 2 step: 308, loss is 0.023037346079945564\n",
      "epoch: 2 step: 309, loss is 0.008792698383331299\n",
      "epoch: 2 step: 310, loss is 0.010210780426859856\n",
      "epoch: 2 step: 311, loss is 0.012054706923663616\n",
      "epoch: 2 step: 312, loss is 0.009899264201521873\n",
      "epoch: 2 step: 313, loss is 0.014808106236159801\n",
      "epoch: 2 step: 314, loss is 0.009355809539556503\n",
      "epoch: 2 step: 315, loss is 0.00618183147162199\n",
      "epoch: 2 step: 316, loss is 0.008858121931552887\n",
      "epoch: 2 step: 317, loss is 0.006183505989611149\n",
      "epoch: 2 step: 318, loss is 0.020941879600286484\n",
      "epoch: 2 step: 319, loss is 0.010483924299478531\n",
      "epoch: 2 step: 320, loss is 0.004470944404602051\n",
      "epoch: 2 step: 321, loss is 0.01106647402048111\n",
      "epoch: 2 step: 322, loss is 0.00432664854452014\n",
      "epoch: 2 step: 323, loss is 0.002864993643015623\n",
      "epoch: 2 step: 324, loss is 0.002371983602643013\n",
      "epoch: 2 step: 325, loss is 0.026086358353495598\n",
      "epoch: 2 step: 326, loss is 0.007739582564681768\n",
      "epoch: 2 step: 327, loss is 0.016164369881153107\n",
      "epoch: 2 step: 328, loss is 0.0318123884499073\n",
      "epoch: 2 step: 329, loss is 0.005621353164315224\n",
      "epoch: 2 step: 330, loss is 0.024259962141513824\n",
      "epoch: 2 step: 331, loss is 0.032453350722789764\n",
      "epoch: 2 step: 332, loss is 0.004293179139494896\n",
      "epoch: 2 step: 333, loss is 0.008130162954330444\n",
      "epoch: 2 step: 334, loss is 0.005575668066740036\n",
      "epoch: 2 step: 335, loss is 0.007406849879771471\n",
      "epoch: 2 step: 336, loss is 0.01560498122125864\n",
      "epoch: 2 step: 337, loss is 0.011292818002402782\n",
      "epoch: 2 step: 338, loss is 0.014860112220048904\n",
      "epoch: 2 step: 339, loss is 0.013258423656225204\n",
      "epoch: 2 step: 340, loss is 0.0264566820114851\n",
      "epoch: 2 step: 341, loss is 0.00947656575590372\n",
      "epoch: 2 step: 342, loss is 0.007773816119879484\n",
      "epoch: 2 step: 343, loss is 0.00689269695430994\n",
      "epoch: 2 step: 344, loss is 0.005101699847728014\n",
      "epoch: 2 step: 345, loss is 0.016845794394612312\n",
      "epoch: 2 step: 346, loss is 0.004336333367973566\n",
      "epoch: 2 step: 347, loss is 0.003419211134314537\n",
      "epoch: 2 step: 348, loss is 0.01055656187236309\n",
      "epoch: 2 step: 349, loss is 0.012206405401229858\n",
      "epoch: 2 step: 350, loss is 0.002686575520783663\n",
      "epoch: 2 step: 351, loss is 0.006181708537042141\n",
      "epoch: 2 step: 352, loss is 0.005929220002144575\n",
      "epoch: 2 step: 353, loss is 0.025152936577796936\n",
      "epoch: 2 step: 354, loss is 0.0026410031132400036\n",
      "epoch: 2 step: 355, loss is 0.011569146998226643\n",
      "epoch: 2 step: 356, loss is 0.0065996996127069\n",
      "epoch: 2 step: 357, loss is 0.008906709030270576\n",
      "epoch: 2 step: 358, loss is 0.014273984357714653\n",
      "epoch: 2 step: 359, loss is 0.0052871135994791985\n",
      "epoch: 2 step: 360, loss is 0.0066588399931788445\n",
      "epoch: 2 step: 361, loss is 0.0027322224341332912\n",
      "epoch: 2 step: 362, loss is 0.01170279923826456\n",
      "epoch: 2 step: 363, loss is 0.01583772338926792\n",
      "epoch: 2 step: 364, loss is 0.016038641333580017\n",
      "epoch: 2 step: 365, loss is 0.007337913848459721\n",
      "epoch: 2 step: 366, loss is 0.008566141128540039\n",
      "epoch: 2 step: 367, loss is 0.03661332279443741\n",
      "epoch: 2 step: 368, loss is 0.010972661897540092\n",
      "epoch: 2 step: 369, loss is 0.003461668733507395\n",
      "epoch: 2 step: 370, loss is 0.01998092234134674\n",
      "epoch: 2 step: 371, loss is 0.0013151109451428056\n",
      "epoch: 2 step: 372, loss is 0.016359549015760422\n",
      "epoch: 2 step: 373, loss is 0.00900895893573761\n",
      "epoch: 2 step: 374, loss is 0.002164633711799979\n",
      "epoch: 2 step: 375, loss is 0.01334268320351839\n",
      "epoch: 2 step: 376, loss is 0.004554418381303549\n",
      "epoch: 2 step: 377, loss is 0.0053564212284982204\n",
      "epoch: 2 step: 378, loss is 0.0074806250631809235\n",
      "epoch: 2 step: 379, loss is 0.022459184750914574\n",
      "epoch: 2 step: 380, loss is 0.0036092449445277452\n",
      "epoch: 2 step: 381, loss is 0.010836616158485413\n",
      "epoch: 2 step: 382, loss is 0.008983882144093513\n",
      "epoch: 2 step: 383, loss is 0.002024044282734394\n",
      "epoch: 2 step: 384, loss is 0.008487803861498833\n",
      "epoch: 2 step: 385, loss is 0.006985581945627928\n",
      "epoch: 2 step: 386, loss is 0.003470831783488393\n",
      "epoch: 2 step: 387, loss is 0.0028104721568524837\n",
      "epoch: 2 step: 388, loss is 0.006914437748491764\n",
      "epoch: 2 step: 389, loss is 0.0024041826836764812\n",
      "epoch: 2 step: 390, loss is 0.0031455871649086475\n",
      "epoch: 2 step: 391, loss is 0.0031985375098884106\n",
      "epoch: 2 step: 392, loss is 0.009306672029197216\n",
      "epoch: 2 step: 393, loss is 0.0060659232549369335\n",
      "epoch: 2 step: 394, loss is 0.003743682987987995\n",
      "epoch: 2 step: 395, loss is 0.004387340042740107\n",
      "epoch: 2 step: 396, loss is 0.003659707959741354\n",
      "epoch: 2 step: 397, loss is 0.0036102868616580963\n",
      "epoch: 2 step: 398, loss is 0.006554713472723961\n",
      "epoch: 2 step: 399, loss is 0.00811472162604332\n",
      "epoch: 2 step: 400, loss is 0.0052526420913636684\n",
      "epoch: 2 step: 401, loss is 0.005088973790407181\n",
      "epoch: 2 step: 402, loss is 0.002661613281816244\n",
      "epoch: 2 step: 403, loss is 0.0044887554831802845\n",
      "epoch: 2 step: 404, loss is 0.016793103888630867\n",
      "epoch: 2 step: 405, loss is 0.005868269130587578\n",
      "epoch: 2 step: 406, loss is 0.0050956630147993565\n",
      "epoch: 2 step: 407, loss is 0.005754869431257248\n",
      "epoch: 2 step: 408, loss is 0.002949112094938755\n",
      "epoch: 2 step: 409, loss is 0.035569123923778534\n",
      "epoch: 2 step: 410, loss is 0.009622866287827492\n",
      "epoch: 2 step: 411, loss is 0.003518700134009123\n",
      "epoch: 2 step: 412, loss is 0.005973643623292446\n",
      "epoch: 2 step: 413, loss is 0.011147752404212952\n",
      "epoch: 2 step: 414, loss is 0.010456160642206669\n",
      "epoch: 2 step: 415, loss is 0.0020290098618716\n",
      "epoch: 2 step: 416, loss is 0.017242498695850372\n",
      "epoch: 2 step: 417, loss is 0.03128315508365631\n",
      "epoch: 2 step: 418, loss is 0.001990244723856449\n",
      "epoch: 2 step: 419, loss is 0.0022420946042984724\n",
      "epoch: 2 step: 420, loss is 0.011813241988420486\n",
      "epoch: 2 step: 421, loss is 0.00392025476321578\n",
      "epoch: 2 step: 422, loss is 0.011337180621922016\n",
      "epoch: 2 step: 423, loss is 0.0031545504461973906\n",
      "epoch: 2 step: 424, loss is 0.011603211984038353\n",
      "epoch: 2 step: 425, loss is 0.01128656230866909\n",
      "epoch: 2 step: 426, loss is 0.007171559147536755\n",
      "epoch: 2 step: 427, loss is 0.008162965066730976\n",
      "epoch: 2 step: 428, loss is 0.003949889913201332\n",
      "epoch: 2 step: 429, loss is 0.003283638507127762\n",
      "epoch: 2 step: 430, loss is 0.01005131658166647\n",
      "epoch: 2 step: 431, loss is 0.003783679101616144\n",
      "epoch: 2 step: 432, loss is 0.005745130591094494\n",
      "epoch: 2 step: 433, loss is 0.006399995647370815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 434, loss is 0.007965873926877975\n",
      "epoch: 2 step: 435, loss is 0.04077021777629852\n",
      "epoch: 2 step: 436, loss is 0.008733784779906273\n",
      "epoch: 2 step: 437, loss is 0.00455778744071722\n",
      "epoch: 2 step: 438, loss is 0.008610434830188751\n",
      "epoch: 2 step: 439, loss is 0.01322388369590044\n",
      "epoch: 2 step: 440, loss is 0.007224894128739834\n",
      "epoch: 2 step: 441, loss is 0.006335665471851826\n",
      "epoch: 2 step: 442, loss is 0.00314535666257143\n",
      "epoch: 2 step: 443, loss is 0.0038643288426101208\n",
      "epoch: 2 step: 444, loss is 0.012292989529669285\n",
      "epoch: 2 step: 445, loss is 0.0024204477667808533\n",
      "epoch: 2 step: 446, loss is 0.006690592039376497\n",
      "epoch: 2 step: 447, loss is 0.004598419182002544\n",
      "epoch: 2 step: 448, loss is 0.004156787879765034\n",
      "epoch: 2 step: 449, loss is 0.011935759335756302\n",
      "epoch: 2 step: 450, loss is 0.002435788046568632\n",
      "epoch: 2 step: 451, loss is 0.0037483624182641506\n",
      "epoch: 2 step: 452, loss is 0.007442277390509844\n",
      "epoch: 2 step: 453, loss is 0.006265407428145409\n",
      "epoch: 2 step: 454, loss is 0.006972433999180794\n",
      "epoch: 2 step: 455, loss is 0.001515835290774703\n",
      "epoch: 2 step: 456, loss is 0.005543162114918232\n",
      "epoch: 2 step: 457, loss is 0.006656026933342218\n",
      "epoch: 2 step: 458, loss is 0.007458910346031189\n",
      "epoch: 2 step: 459, loss is 0.004714664537459612\n",
      "epoch: 2 step: 460, loss is 0.007624805439263582\n",
      "epoch: 2 step: 461, loss is 0.005537119228392839\n",
      "epoch: 2 step: 462, loss is 0.008972058072686195\n",
      "epoch: 2 step: 463, loss is 0.0016217520460486412\n",
      "epoch: 2 step: 464, loss is 0.003964811563491821\n",
      "epoch: 2 step: 465, loss is 0.005009816028177738\n",
      "epoch: 2 step: 466, loss is 0.0032589349430054426\n",
      "epoch: 2 step: 467, loss is 0.004893373232334852\n",
      "epoch: 2 step: 468, loss is 0.0022673653438687325\n",
      "epoch: 2 step: 469, loss is 0.0029936479404568672\n",
      "epoch: 2 step: 470, loss is 0.003533324459567666\n",
      "epoch: 2 step: 471, loss is 0.0019258749671280384\n",
      "epoch: 2 step: 472, loss is 0.001446289592422545\n",
      "epoch: 2 step: 473, loss is 0.001235805219039321\n",
      "epoch: 2 step: 474, loss is 0.013537047430872917\n",
      "epoch: 2 step: 475, loss is 0.009975137189030647\n",
      "epoch: 2 step: 476, loss is 0.006572015583515167\n",
      "epoch: 2 step: 477, loss is 0.010866507887840271\n",
      "epoch: 2 step: 478, loss is 0.0027722888626158237\n",
      "epoch: 2 step: 479, loss is 0.0032480345107614994\n",
      "epoch: 2 step: 480, loss is 0.017878269776701927\n",
      "epoch: 2 step: 481, loss is 0.002806901466101408\n",
      "epoch: 2 step: 482, loss is 0.006934708915650845\n",
      "epoch: 2 step: 483, loss is 0.009026484563946724\n",
      "epoch: 2 step: 484, loss is 0.008331634104251862\n",
      "epoch: 2 step: 485, loss is 0.005327802151441574\n",
      "epoch: 2 step: 486, loss is 0.005798017140477896\n",
      "epoch: 2 step: 487, loss is 0.0031681773252785206\n",
      "epoch: 2 step: 488, loss is 0.0045744129456579685\n",
      "epoch: 2 step: 489, loss is 0.006269359029829502\n",
      "epoch: 2 step: 490, loss is 0.0019273585639894009\n",
      "epoch: 2 step: 491, loss is 0.005972472485154867\n",
      "epoch: 2 step: 492, loss is 0.00969449058175087\n",
      "epoch: 2 step: 493, loss is 0.005972892511636019\n",
      "epoch: 2 step: 494, loss is 0.011525223962962627\n",
      "epoch: 2 step: 495, loss is 0.0111150611191988\n",
      "epoch: 2 step: 496, loss is 0.00131509720813483\n",
      "epoch: 2 step: 497, loss is 0.0075294082053005695\n",
      "epoch: 2 step: 498, loss is 0.0017444111872464418\n",
      "epoch: 2 step: 499, loss is 0.00221440801396966\n",
      "epoch: 2 step: 500, loss is 0.0017502043629065156\n",
      "epoch: 2 step: 501, loss is 0.006984126754105091\n",
      "epoch: 2 step: 502, loss is 0.012878111563622952\n",
      "epoch: 2 step: 503, loss is 0.002528903540223837\n",
      "epoch: 2 step: 504, loss is 0.0017315777949988842\n",
      "epoch: 2 step: 505, loss is 0.001400413573719561\n",
      "epoch: 2 step: 506, loss is 0.0026002605445683002\n",
      "epoch: 2 step: 507, loss is 0.0025994630996137857\n",
      "epoch: 2 step: 508, loss is 0.001513889292255044\n",
      "epoch: 2 step: 509, loss is 0.003005866426974535\n",
      "epoch: 2 step: 510, loss is 0.0004598218365572393\n",
      "epoch: 2 step: 511, loss is 0.005256365519016981\n",
      "epoch: 2 step: 512, loss is 0.008605970069766045\n",
      "epoch: 2 step: 513, loss is 0.008462822064757347\n",
      "epoch: 2 step: 514, loss is 0.01779545657336712\n",
      "epoch: 2 step: 515, loss is 0.00437858235090971\n",
      "epoch: 2 step: 516, loss is 0.00811827089637518\n",
      "epoch: 2 step: 517, loss is 0.002602183958515525\n",
      "epoch: 2 step: 518, loss is 0.002451948821544647\n",
      "epoch: 2 step: 519, loss is 0.0030946978367865086\n",
      "epoch: 2 step: 520, loss is 0.0013611209578812122\n",
      "epoch: 2 step: 521, loss is 0.0038049074355512857\n",
      "epoch: 2 step: 522, loss is 0.0037012342363595963\n",
      "epoch: 2 step: 523, loss is 0.0016228187596425414\n",
      "epoch: 2 step: 524, loss is 0.002763503696769476\n",
      "epoch: 2 step: 525, loss is 0.0023340056650340557\n",
      "epoch: 2 step: 526, loss is 0.004921965766698122\n",
      "epoch: 2 step: 527, loss is 0.002960875164717436\n",
      "epoch: 2 step: 528, loss is 0.005245395004749298\n",
      "epoch: 2 step: 529, loss is 0.0030751992017030716\n",
      "epoch: 2 step: 530, loss is 0.005461675114929676\n",
      "epoch: 2 step: 531, loss is 0.006529792211949825\n",
      "epoch: 2 step: 532, loss is 0.0011680268216878176\n",
      "epoch: 2 step: 533, loss is 0.0023916722275316715\n",
      "epoch: 2 step: 534, loss is 0.02137215994298458\n",
      "epoch: 2 step: 535, loss is 0.007363485172390938\n",
      "epoch: 2 step: 536, loss is 0.003151283599436283\n",
      "epoch: 2 step: 537, loss is 0.0033670123666524887\n",
      "epoch: 2 step: 538, loss is 0.001521892030723393\n",
      "epoch: 2 step: 539, loss is 0.002637946978211403\n",
      "epoch: 2 step: 540, loss is 0.005211297422647476\n",
      "epoch: 2 step: 541, loss is 0.002916942350566387\n",
      "epoch: 2 step: 542, loss is 0.004011842422187328\n",
      "epoch: 2 step: 543, loss is 0.0010025770170614123\n",
      "epoch: 2 step: 544, loss is 0.0015250345459207892\n",
      "epoch: 2 step: 545, loss is 0.002396214520558715\n",
      "epoch: 2 step: 546, loss is 0.00380936311557889\n",
      "epoch: 2 step: 547, loss is 0.002670629182830453\n",
      "epoch: 2 step: 548, loss is 0.009381381794810295\n",
      "epoch: 2 step: 549, loss is 0.004964183550328016\n",
      "epoch: 2 step: 550, loss is 0.0037497449666261673\n",
      "epoch: 2 step: 551, loss is 0.0027182872872799635\n",
      "epoch: 2 step: 552, loss is 0.004176076035946608\n",
      "epoch: 2 step: 553, loss is 0.013553251512348652\n",
      "epoch: 2 step: 554, loss is 0.002631189301609993\n",
      "epoch: 2 step: 555, loss is 0.002964853076264262\n",
      "epoch: 2 step: 556, loss is 0.004152595531195402\n",
      "epoch: 2 step: 557, loss is 0.0033587459474802017\n",
      "epoch: 2 step: 558, loss is 0.007633059285581112\n",
      "epoch: 2 step: 559, loss is 0.002259455155581236\n",
      "epoch: 2 step: 560, loss is 0.0031200903467833996\n",
      "epoch: 2 step: 561, loss is 0.0036806841380894184\n",
      "epoch: 2 step: 562, loss is 0.0019354844698682427\n",
      "epoch: 2 step: 563, loss is 0.0022780795115977526\n",
      "epoch: 2 step: 564, loss is 0.0047465437091887\n",
      "epoch: 2 step: 565, loss is 0.006568389944732189\n",
      "epoch: 2 step: 566, loss is 0.00198739068582654\n",
      "epoch: 2 step: 567, loss is 0.0007486427202820778\n",
      "epoch: 2 step: 568, loss is 0.0008840038790367544\n",
      "epoch: 2 step: 569, loss is 0.0020062157418578863\n",
      "epoch: 2 step: 570, loss is 0.0021212429273873568\n",
      "epoch: 2 step: 571, loss is 0.001104371272958815\n",
      "epoch: 2 step: 572, loss is 0.003034120425581932\n",
      "epoch: 2 step: 573, loss is 0.006100242026150227\n",
      "epoch: 2 step: 574, loss is 0.0056499033235013485\n",
      "epoch: 2 step: 575, loss is 0.005859689321368933\n",
      "epoch: 2 step: 576, loss is 0.0061370572075247765\n",
      "epoch: 2 step: 577, loss is 0.0016233236528933048\n",
      "epoch: 2 step: 578, loss is 0.002258964814245701\n",
      "epoch: 2 step: 579, loss is 0.015398124232888222\n",
      "epoch: 2 step: 580, loss is 0.006535565480589867\n",
      "epoch: 2 step: 581, loss is 0.002052642870694399\n",
      "epoch: 2 step: 582, loss is 0.0008828883874230087\n",
      "epoch: 2 step: 583, loss is 0.0019321274012327194\n",
      "epoch: 2 step: 584, loss is 0.0010794552508741617\n",
      "epoch: 2 step: 585, loss is 0.002836803440004587\n",
      "epoch: 2 step: 586, loss is 0.006052668206393719\n",
      "epoch: 2 step: 587, loss is 0.004405970685184002\n",
      "epoch: 2 step: 588, loss is 0.005608369130641222\n",
      "epoch: 2 step: 589, loss is 0.002977983560413122\n",
      "epoch: 2 step: 590, loss is 0.001664092531427741\n",
      "epoch: 2 step: 591, loss is 0.0011482792906463146\n",
      "epoch: 2 step: 592, loss is 0.0007744384929537773\n",
      "epoch: 2 step: 593, loss is 0.009814982302486897\n",
      "epoch: 2 step: 594, loss is 0.003019892144948244\n",
      "epoch: 2 step: 595, loss is 0.0031638559885323048\n",
      "epoch: 2 step: 596, loss is 0.0034297711681574583\n",
      "Train epoch time: 29873.834 ms, per step time: 50.124 ms\n",
      "epoch: 3 step: 1, loss is 0.004246116615831852\n",
      "epoch: 3 step: 2, loss is 0.001619590213522315\n",
      "epoch: 3 step: 3, loss is 0.001271642162464559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 4, loss is 0.006485814228653908\n",
      "epoch: 3 step: 5, loss is 0.005310133099555969\n",
      "epoch: 3 step: 6, loss is 0.002869605552405119\n",
      "epoch: 3 step: 7, loss is 0.0015485782641917467\n",
      "epoch: 3 step: 8, loss is 0.0019053402356803417\n",
      "epoch: 3 step: 9, loss is 0.0021884250454604626\n",
      "epoch: 3 step: 10, loss is 0.0056383078917860985\n",
      "epoch: 3 step: 11, loss is 0.005306471139192581\n",
      "epoch: 3 step: 12, loss is 0.006432119756937027\n",
      "epoch: 3 step: 13, loss is 0.006602785550057888\n",
      "epoch: 3 step: 14, loss is 0.004483181983232498\n",
      "epoch: 3 step: 15, loss is 0.0036771465092897415\n",
      "epoch: 3 step: 16, loss is 0.0021197593305259943\n",
      "epoch: 3 step: 17, loss is 0.0035662224981933832\n",
      "epoch: 3 step: 18, loss is 0.0023990324698388577\n",
      "epoch: 3 step: 19, loss is 0.0025927869137376547\n",
      "epoch: 3 step: 20, loss is 0.005218863021582365\n",
      "epoch: 3 step: 21, loss is 0.0009511513635516167\n",
      "epoch: 3 step: 22, loss is 0.001463944325223565\n",
      "epoch: 3 step: 23, loss is 0.002514293882995844\n",
      "epoch: 3 step: 24, loss is 0.0008530057966709137\n",
      "epoch: 3 step: 25, loss is 0.0037077292799949646\n",
      "epoch: 3 step: 26, loss is 0.0024815259966999292\n",
      "epoch: 3 step: 27, loss is 0.005334699526429176\n",
      "epoch: 3 step: 28, loss is 0.003618380520492792\n",
      "epoch: 3 step: 29, loss is 0.004414881579577923\n",
      "epoch: 3 step: 30, loss is 0.005229483358561993\n",
      "epoch: 3 step: 31, loss is 0.0009508488583378494\n",
      "epoch: 3 step: 32, loss is 0.002166192280128598\n",
      "epoch: 3 step: 33, loss is 0.007738778367638588\n",
      "epoch: 3 step: 34, loss is 0.0015100479358807206\n",
      "epoch: 3 step: 35, loss is 0.0024761308450251818\n",
      "epoch: 3 step: 36, loss is 0.0025113755837082863\n",
      "epoch: 3 step: 37, loss is 0.0029828641563653946\n",
      "epoch: 3 step: 38, loss is 0.004397390875965357\n",
      "epoch: 3 step: 39, loss is 0.002009724033996463\n",
      "epoch: 3 step: 40, loss is 0.00256953202188015\n",
      "epoch: 3 step: 41, loss is 0.00634096609428525\n",
      "epoch: 3 step: 42, loss is 0.003757460741326213\n",
      "epoch: 3 step: 43, loss is 0.0008600721484981477\n",
      "epoch: 3 step: 44, loss is 0.005730535369366407\n",
      "epoch: 3 step: 45, loss is 0.0015342481201514602\n",
      "epoch: 3 step: 46, loss is 0.0029273994732648134\n",
      "epoch: 3 step: 47, loss is 0.007374380249530077\n",
      "epoch: 3 step: 48, loss is 0.0011735879816114902\n",
      "epoch: 3 step: 49, loss is 0.001511517446488142\n",
      "epoch: 3 step: 50, loss is 0.0026201673317700624\n",
      "epoch: 3 step: 51, loss is 0.0009371192427352071\n",
      "epoch: 3 step: 52, loss is 0.000763901392929256\n",
      "epoch: 3 step: 53, loss is 0.0014896322973072529\n",
      "epoch: 3 step: 54, loss is 0.0036002029664814472\n",
      "epoch: 3 step: 55, loss is 0.009147107601165771\n",
      "epoch: 3 step: 56, loss is 0.0013504420639947057\n",
      "epoch: 3 step: 57, loss is 0.0029201763682067394\n",
      "epoch: 3 step: 58, loss is 0.0009058053256012499\n",
      "epoch: 3 step: 59, loss is 0.0013290713541209698\n",
      "epoch: 3 step: 60, loss is 0.003591519547626376\n",
      "epoch: 3 step: 61, loss is 0.001108180033043027\n",
      "epoch: 3 step: 62, loss is 0.01043294183909893\n",
      "epoch: 3 step: 63, loss is 0.0009283602703362703\n",
      "epoch: 3 step: 64, loss is 0.0016110966680571437\n",
      "epoch: 3 step: 65, loss is 0.004619996063411236\n",
      "epoch: 3 step: 66, loss is 0.0033614858984947205\n",
      "epoch: 3 step: 67, loss is 0.0019792472012341022\n",
      "epoch: 3 step: 68, loss is 0.0012043127790093422\n",
      "epoch: 3 step: 69, loss is 0.00642214622348547\n",
      "epoch: 3 step: 70, loss is 0.0017301295883953571\n",
      "epoch: 3 step: 71, loss is 0.0028133303858339787\n",
      "epoch: 3 step: 72, loss is 0.003301307326182723\n",
      "epoch: 3 step: 73, loss is 0.0009637253824621439\n",
      "epoch: 3 step: 74, loss is 0.0030726175755262375\n",
      "epoch: 3 step: 75, loss is 0.005060157272964716\n",
      "epoch: 3 step: 76, loss is 0.0009217001497745514\n",
      "epoch: 3 step: 77, loss is 0.001871192827820778\n",
      "epoch: 3 step: 78, loss is 0.0032635799143463373\n",
      "epoch: 3 step: 79, loss is 0.019476372748613358\n",
      "epoch: 3 step: 80, loss is 0.002418621676042676\n",
      "epoch: 3 step: 81, loss is 0.006464502774178982\n",
      "epoch: 3 step: 82, loss is 0.0014591449871659279\n",
      "epoch: 3 step: 83, loss is 0.0035144961439073086\n",
      "epoch: 3 step: 84, loss is 0.005901408847421408\n",
      "epoch: 3 step: 85, loss is 0.001119782100431621\n",
      "epoch: 3 step: 86, loss is 0.003223211970180273\n",
      "epoch: 3 step: 87, loss is 0.0250608641654253\n",
      "epoch: 3 step: 88, loss is 0.0010497055482119322\n",
      "epoch: 3 step: 89, loss is 0.0009967500809580088\n",
      "epoch: 3 step: 90, loss is 0.0016222374979406595\n",
      "epoch: 3 step: 91, loss is 0.0022356065455824137\n",
      "epoch: 3 step: 92, loss is 0.0019008098170161247\n",
      "epoch: 3 step: 93, loss is 0.0012607565149664879\n",
      "epoch: 3 step: 94, loss is 0.0009603549260646105\n",
      "epoch: 3 step: 95, loss is 0.0023138076066970825\n",
      "epoch: 3 step: 96, loss is 0.0010216373484581709\n",
      "epoch: 3 step: 97, loss is 0.001163471955806017\n",
      "epoch: 3 step: 98, loss is 0.001271864166483283\n",
      "epoch: 3 step: 99, loss is 0.002184135839343071\n",
      "epoch: 3 step: 100, loss is 0.0014654304832220078\n",
      "epoch: 3 step: 101, loss is 0.003347844583913684\n",
      "epoch: 3 step: 102, loss is 0.0027083202730864286\n",
      "epoch: 3 step: 103, loss is 0.0021129557862877846\n",
      "epoch: 3 step: 104, loss is 0.000685646606143564\n",
      "epoch: 3 step: 105, loss is 0.0025739397387951612\n",
      "epoch: 3 step: 106, loss is 0.010825889185070992\n",
      "epoch: 3 step: 107, loss is 0.0024655205197632313\n",
      "epoch: 3 step: 108, loss is 0.0023778709582984447\n",
      "epoch: 3 step: 109, loss is 0.002333339536562562\n",
      "epoch: 3 step: 110, loss is 0.0017262207111343741\n",
      "epoch: 3 step: 111, loss is 0.006859065033495426\n",
      "epoch: 3 step: 112, loss is 0.0019368268549442291\n",
      "epoch: 3 step: 113, loss is 0.0017197157721966505\n",
      "epoch: 3 step: 114, loss is 0.004149567801505327\n",
      "epoch: 3 step: 115, loss is 0.001577449613250792\n",
      "epoch: 3 step: 116, loss is 0.0014159348793327808\n",
      "epoch: 3 step: 117, loss is 0.0034730914048850536\n",
      "epoch: 3 step: 118, loss is 0.0032974863424897194\n",
      "epoch: 3 step: 119, loss is 0.0017617904813960195\n",
      "epoch: 3 step: 120, loss is 0.0008347042021341622\n",
      "epoch: 3 step: 121, loss is 0.0010430028196424246\n",
      "epoch: 3 step: 122, loss is 0.0011446719290688634\n",
      "epoch: 3 step: 123, loss is 0.0014744737418368459\n",
      "epoch: 3 step: 124, loss is 0.00039430501055903733\n",
      "epoch: 3 step: 125, loss is 0.0008865782292559743\n",
      "epoch: 3 step: 126, loss is 0.0014926434960216284\n",
      "epoch: 3 step: 127, loss is 0.0010993119794875383\n",
      "epoch: 3 step: 128, loss is 0.0021781958639621735\n",
      "epoch: 3 step: 129, loss is 0.0024897917173802853\n",
      "epoch: 3 step: 130, loss is 0.0021718237549066544\n",
      "epoch: 3 step: 131, loss is 0.0011571766808629036\n",
      "epoch: 3 step: 132, loss is 0.005516366101801395\n",
      "epoch: 3 step: 133, loss is 0.0019415502902120352\n",
      "epoch: 3 step: 134, loss is 0.0020096460357308388\n",
      "epoch: 3 step: 135, loss is 0.0008489990141242743\n",
      "epoch: 3 step: 136, loss is 0.002348591573536396\n",
      "epoch: 3 step: 137, loss is 0.0023498437367379665\n",
      "epoch: 3 step: 138, loss is 0.001385643845424056\n",
      "epoch: 3 step: 139, loss is 0.0025885174982249737\n",
      "epoch: 3 step: 140, loss is 0.0013592196628451347\n",
      "epoch: 3 step: 141, loss is 0.00145907502155751\n",
      "epoch: 3 step: 142, loss is 0.0020652173552662134\n",
      "epoch: 3 step: 143, loss is 0.005891913082450628\n",
      "epoch: 3 step: 144, loss is 0.0016383054899051785\n",
      "epoch: 3 step: 145, loss is 0.0009677486377768219\n",
      "epoch: 3 step: 146, loss is 0.00687837740406394\n",
      "epoch: 3 step: 147, loss is 0.025411000475287437\n",
      "epoch: 3 step: 148, loss is 0.0017938587116077542\n",
      "epoch: 3 step: 149, loss is 0.002036536578088999\n",
      "epoch: 3 step: 150, loss is 0.004191691055893898\n",
      "epoch: 3 step: 151, loss is 0.0019261128036305308\n",
      "epoch: 3 step: 152, loss is 0.008072937838733196\n",
      "epoch: 3 step: 153, loss is 0.00100949895568192\n",
      "epoch: 3 step: 154, loss is 0.002082370687276125\n",
      "epoch: 3 step: 155, loss is 0.0009297417127527297\n",
      "epoch: 3 step: 156, loss is 0.0021138619631528854\n",
      "epoch: 3 step: 157, loss is 0.0016792649403214455\n",
      "epoch: 3 step: 158, loss is 0.0014049611054360867\n",
      "epoch: 3 step: 159, loss is 0.0031546135433018208\n",
      "epoch: 3 step: 160, loss is 0.0015508048236370087\n",
      "epoch: 3 step: 161, loss is 0.0010964676039293408\n",
      "epoch: 3 step: 162, loss is 0.0024508603382855654\n",
      "epoch: 3 step: 163, loss is 0.0012010422069579363\n",
      "epoch: 3 step: 164, loss is 0.0014760539634153247\n",
      "epoch: 3 step: 165, loss is 0.0005520143313333392\n",
      "epoch: 3 step: 166, loss is 0.0013967837439849973\n",
      "epoch: 3 step: 167, loss is 0.001956230727955699\n",
      "epoch: 3 step: 168, loss is 0.0017702431650832295\n",
      "epoch: 3 step: 169, loss is 0.0018840149277821183\n",
      "epoch: 3 step: 170, loss is 0.0008228207589127123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 171, loss is 0.0005666576325893402\n",
      "epoch: 3 step: 172, loss is 0.001007279846817255\n",
      "epoch: 3 step: 173, loss is 0.00046625779941678047\n",
      "epoch: 3 step: 174, loss is 0.001139873987995088\n",
      "epoch: 3 step: 175, loss is 0.0010595516068860888\n",
      "epoch: 3 step: 176, loss is 0.0038915914483368397\n",
      "epoch: 3 step: 177, loss is 0.0017088211607187986\n",
      "epoch: 3 step: 178, loss is 0.002161039039492607\n",
      "epoch: 3 step: 179, loss is 0.009596004150807858\n",
      "epoch: 3 step: 180, loss is 0.0008852881146594882\n",
      "epoch: 3 step: 181, loss is 0.0008965842425823212\n",
      "epoch: 3 step: 182, loss is 0.003184873377904296\n",
      "epoch: 3 step: 183, loss is 0.0010544852120801806\n",
      "epoch: 3 step: 184, loss is 0.0023334925062954426\n",
      "epoch: 3 step: 185, loss is 0.000971881439909339\n",
      "epoch: 3 step: 186, loss is 0.001969818025827408\n",
      "epoch: 3 step: 187, loss is 0.00208727503195405\n",
      "epoch: 3 step: 188, loss is 0.0008612776873633265\n",
      "epoch: 3 step: 189, loss is 0.0012381526175886393\n",
      "epoch: 3 step: 190, loss is 0.0038757063448429108\n",
      "epoch: 3 step: 191, loss is 0.004543004557490349\n",
      "epoch: 3 step: 192, loss is 0.001240541459992528\n",
      "epoch: 3 step: 193, loss is 0.0014249372761696577\n",
      "epoch: 3 step: 194, loss is 0.0008543450385332108\n",
      "epoch: 3 step: 195, loss is 0.0016031543491408229\n",
      "epoch: 3 step: 196, loss is 0.0026966328732669353\n",
      "epoch: 3 step: 197, loss is 0.0005645654746331275\n",
      "epoch: 3 step: 198, loss is 0.0005577591364271939\n",
      "epoch: 3 step: 199, loss is 0.0019668263848870993\n",
      "epoch: 3 step: 200, loss is 0.003520652186125517\n",
      "epoch: 3 step: 201, loss is 0.0008916013175621629\n",
      "epoch: 3 step: 202, loss is 0.0012511520180851221\n",
      "epoch: 3 step: 203, loss is 0.0013975786278024316\n",
      "epoch: 3 step: 204, loss is 0.005381176248192787\n",
      "epoch: 3 step: 205, loss is 0.0011626083869487047\n",
      "epoch: 3 step: 206, loss is 0.0007819248712621629\n",
      "epoch: 3 step: 207, loss is 0.000558065134100616\n",
      "epoch: 3 step: 208, loss is 0.00080341991269961\n",
      "epoch: 3 step: 209, loss is 0.0011957999086007476\n",
      "epoch: 3 step: 210, loss is 0.0007248410256579518\n",
      "epoch: 3 step: 211, loss is 0.001618602080270648\n",
      "epoch: 3 step: 212, loss is 0.00030416378285735846\n",
      "epoch: 3 step: 213, loss is 0.0072938003577291965\n",
      "epoch: 3 step: 214, loss is 0.0032224131282418966\n",
      "epoch: 3 step: 215, loss is 0.002290346659719944\n",
      "epoch: 3 step: 216, loss is 0.0012805314036086202\n",
      "epoch: 3 step: 217, loss is 0.001507240580394864\n",
      "epoch: 3 step: 218, loss is 0.00226524006575346\n",
      "epoch: 3 step: 219, loss is 0.0011332833673804998\n",
      "epoch: 3 step: 220, loss is 0.0008425394771620631\n",
      "epoch: 3 step: 221, loss is 0.002260764827951789\n",
      "epoch: 3 step: 222, loss is 0.0006802077405154705\n",
      "epoch: 3 step: 223, loss is 0.003066610312089324\n",
      "epoch: 3 step: 224, loss is 0.0014050663448870182\n",
      "epoch: 3 step: 225, loss is 0.0006609257543459535\n",
      "epoch: 3 step: 226, loss is 0.0023838882334530354\n",
      "epoch: 3 step: 227, loss is 0.0013225823640823364\n",
      "epoch: 3 step: 228, loss is 0.0009321051766164601\n",
      "epoch: 3 step: 229, loss is 0.0017904232954606414\n",
      "epoch: 3 step: 230, loss is 0.0013583134859800339\n",
      "epoch: 3 step: 231, loss is 0.001082848059013486\n",
      "epoch: 3 step: 232, loss is 0.0012213097652420402\n",
      "epoch: 3 step: 233, loss is 0.001285308739170432\n",
      "epoch: 3 step: 234, loss is 0.0010002206545323133\n",
      "epoch: 3 step: 235, loss is 0.001784792053513229\n",
      "epoch: 3 step: 236, loss is 0.0024305139668285847\n",
      "epoch: 3 step: 237, loss is 0.0014817044138908386\n",
      "epoch: 3 step: 238, loss is 0.0011978038819506764\n",
      "epoch: 3 step: 239, loss is 0.0012125573121011257\n",
      "epoch: 3 step: 240, loss is 0.00047633290523663163\n",
      "epoch: 3 step: 241, loss is 0.0007226776797324419\n",
      "epoch: 3 step: 242, loss is 0.0017210827209055424\n",
      "epoch: 3 step: 243, loss is 0.0006907371571287513\n",
      "epoch: 3 step: 244, loss is 0.00238406122662127\n",
      "epoch: 3 step: 245, loss is 0.0008223990444093943\n",
      "epoch: 3 step: 246, loss is 0.0007417785818688571\n",
      "epoch: 3 step: 247, loss is 0.0008965044980868697\n",
      "epoch: 3 step: 248, loss is 0.0011113209184259176\n",
      "epoch: 3 step: 249, loss is 0.0017078210366889834\n",
      "epoch: 3 step: 250, loss is 0.002478961367160082\n",
      "epoch: 3 step: 251, loss is 0.002172645414248109\n",
      "epoch: 3 step: 252, loss is 0.0012363770511001348\n",
      "epoch: 3 step: 253, loss is 0.0006140294135548174\n",
      "epoch: 3 step: 254, loss is 0.0020393594168126583\n",
      "epoch: 3 step: 255, loss is 0.010238904505968094\n",
      "epoch: 3 step: 256, loss is 0.0017551700584590435\n",
      "epoch: 3 step: 257, loss is 0.000865850830450654\n",
      "epoch: 3 step: 258, loss is 0.001147598261013627\n",
      "epoch: 3 step: 259, loss is 0.0009988141246140003\n",
      "epoch: 3 step: 260, loss is 0.0018259936477988958\n",
      "epoch: 3 step: 261, loss is 0.0009402786381542683\n",
      "epoch: 3 step: 262, loss is 0.001222612103447318\n",
      "epoch: 3 step: 263, loss is 0.002931094029918313\n",
      "epoch: 3 step: 264, loss is 0.0017433393513783813\n",
      "epoch: 3 step: 265, loss is 0.0008394438773393631\n",
      "epoch: 3 step: 266, loss is 0.00027522415621206164\n",
      "epoch: 3 step: 267, loss is 0.001993255689740181\n",
      "epoch: 3 step: 268, loss is 0.0011528203031048179\n",
      "epoch: 3 step: 269, loss is 0.001379834022372961\n",
      "epoch: 3 step: 270, loss is 0.0003920468734577298\n",
      "epoch: 3 step: 271, loss is 0.003518067765980959\n",
      "epoch: 3 step: 272, loss is 0.0012843362055718899\n",
      "epoch: 3 step: 273, loss is 0.00046438706340268254\n",
      "epoch: 3 step: 274, loss is 0.00045201435568742454\n",
      "epoch: 3 step: 275, loss is 0.0019020781619474292\n",
      "epoch: 3 step: 276, loss is 0.0007177923107519746\n",
      "epoch: 3 step: 277, loss is 0.0025304900482296944\n",
      "epoch: 3 step: 278, loss is 0.0027570128440856934\n",
      "epoch: 3 step: 279, loss is 0.003003383055329323\n",
      "epoch: 3 step: 280, loss is 0.0023564891889691353\n",
      "epoch: 3 step: 281, loss is 0.0013941311044618487\n",
      "epoch: 3 step: 282, loss is 0.0012116903671994805\n",
      "epoch: 3 step: 283, loss is 0.0019973362796008587\n",
      "epoch: 3 step: 284, loss is 0.0008608120260760188\n",
      "epoch: 3 step: 285, loss is 0.0014420587103813887\n",
      "epoch: 3 step: 286, loss is 0.0018474083626642823\n",
      "epoch: 3 step: 287, loss is 0.0007865948136895895\n",
      "epoch: 3 step: 288, loss is 0.003083900548517704\n",
      "epoch: 3 step: 289, loss is 0.0018560318276286125\n",
      "epoch: 3 step: 290, loss is 0.005499907303601503\n",
      "epoch: 3 step: 291, loss is 0.000586539157666266\n",
      "epoch: 3 step: 292, loss is 0.0005843465914949775\n",
      "epoch: 3 step: 293, loss is 0.0005531827919185162\n",
      "epoch: 3 step: 294, loss is 0.00122549245133996\n",
      "epoch: 3 step: 295, loss is 0.002866933820769191\n",
      "epoch: 3 step: 296, loss is 0.0008076820522546768\n",
      "epoch: 3 step: 297, loss is 0.0027123799081891775\n",
      "epoch: 3 step: 298, loss is 0.0012674370082095265\n",
      "epoch: 3 step: 299, loss is 0.0017268586670979857\n",
      "epoch: 3 step: 300, loss is 0.0006749881431460381\n",
      "epoch: 3 step: 301, loss is 0.0005186306079849601\n",
      "epoch: 3 step: 302, loss is 0.00045841478276997805\n",
      "epoch: 3 step: 303, loss is 0.001841887948103249\n",
      "epoch: 3 step: 304, loss is 0.0012804789002984762\n",
      "epoch: 3 step: 305, loss is 0.0006688310531899333\n",
      "epoch: 3 step: 306, loss is 0.0003057850990444422\n",
      "epoch: 3 step: 307, loss is 0.0023173168301582336\n",
      "epoch: 3 step: 308, loss is 0.004194941371679306\n",
      "epoch: 3 step: 309, loss is 0.0019442138727754354\n",
      "epoch: 3 step: 310, loss is 0.0009582466445863247\n",
      "epoch: 3 step: 311, loss is 0.0008027443545870483\n",
      "epoch: 3 step: 312, loss is 0.002188060898333788\n",
      "epoch: 3 step: 313, loss is 0.0008021965622901917\n",
      "epoch: 3 step: 314, loss is 0.00028058316092938185\n",
      "epoch: 3 step: 315, loss is 0.0009839918930083513\n",
      "epoch: 3 step: 316, loss is 0.002509016077965498\n",
      "epoch: 3 step: 317, loss is 0.0012432015500962734\n",
      "epoch: 3 step: 318, loss is 0.0019312106305733323\n",
      "epoch: 3 step: 319, loss is 0.0007464109221473336\n",
      "epoch: 3 step: 320, loss is 0.0006458463612943888\n",
      "epoch: 3 step: 321, loss is 0.0003536901785992086\n",
      "epoch: 3 step: 322, loss is 0.0005970264901407063\n",
      "epoch: 3 step: 323, loss is 0.00030742675880901515\n",
      "epoch: 3 step: 324, loss is 0.0007179146050475538\n",
      "epoch: 3 step: 325, loss is 0.0023437021300196648\n",
      "epoch: 3 step: 326, loss is 0.0007845512009225786\n",
      "epoch: 3 step: 327, loss is 0.001361885224469006\n",
      "epoch: 3 step: 328, loss is 0.0031094008591026068\n",
      "epoch: 3 step: 329, loss is 0.0003566828090697527\n",
      "epoch: 3 step: 330, loss is 0.000654522911645472\n",
      "epoch: 3 step: 331, loss is 0.005173464305698872\n",
      "epoch: 3 step: 332, loss is 0.00033014488872140646\n",
      "epoch: 3 step: 333, loss is 0.0027170097455382347\n",
      "epoch: 3 step: 334, loss is 0.0012085383059456944\n",
      "epoch: 3 step: 335, loss is 0.0013162292307242751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 336, loss is 0.0010563535615801811\n",
      "epoch: 3 step: 337, loss is 0.0015026233159005642\n",
      "epoch: 3 step: 338, loss is 0.0011576383840292692\n",
      "epoch: 3 step: 339, loss is 0.0011942198034375906\n",
      "epoch: 3 step: 340, loss is 0.006865449249744415\n",
      "epoch: 3 step: 341, loss is 0.0005099551635794342\n",
      "epoch: 3 step: 342, loss is 0.002866553608328104\n",
      "epoch: 3 step: 343, loss is 0.0013312563532963395\n",
      "epoch: 3 step: 344, loss is 0.0013960963115096092\n",
      "epoch: 3 step: 345, loss is 0.0031881940085440874\n",
      "epoch: 3 step: 346, loss is 0.0005423207185231149\n",
      "epoch: 3 step: 347, loss is 0.0005054677021689713\n",
      "epoch: 3 step: 348, loss is 0.0011283046333119273\n",
      "epoch: 3 step: 349, loss is 0.0007520875660702586\n",
      "epoch: 3 step: 350, loss is 0.0003019528812728822\n",
      "epoch: 3 step: 351, loss is 0.0027607937809079885\n",
      "epoch: 3 step: 352, loss is 0.0035061510279774666\n",
      "epoch: 3 step: 353, loss is 0.004004169255495071\n",
      "epoch: 3 step: 354, loss is 0.00048367702402174473\n",
      "epoch: 3 step: 355, loss is 0.0004469208943191916\n",
      "epoch: 3 step: 356, loss is 0.0002735246089287102\n",
      "epoch: 3 step: 357, loss is 0.00041519245132803917\n",
      "epoch: 3 step: 358, loss is 0.002491411054506898\n",
      "epoch: 3 step: 359, loss is 0.0023772050626575947\n",
      "epoch: 3 step: 360, loss is 0.0013195282081142068\n",
      "epoch: 3 step: 361, loss is 0.0007925178506411612\n",
      "epoch: 3 step: 362, loss is 0.0009397222311235964\n",
      "epoch: 3 step: 363, loss is 0.0014425558038055897\n",
      "epoch: 3 step: 364, loss is 0.002090051770210266\n",
      "epoch: 3 step: 365, loss is 0.002814039122313261\n",
      "epoch: 3 step: 366, loss is 0.003212666604667902\n",
      "epoch: 3 step: 367, loss is 0.00254385476000607\n",
      "epoch: 3 step: 368, loss is 0.004559908527880907\n",
      "epoch: 3 step: 369, loss is 0.0008291559643112123\n",
      "epoch: 3 step: 370, loss is 0.0026943497359752655\n",
      "epoch: 3 step: 371, loss is 0.00037447430077008903\n",
      "epoch: 3 step: 372, loss is 0.0022104368545114994\n",
      "epoch: 3 step: 373, loss is 0.0007031415589153767\n",
      "epoch: 3 step: 374, loss is 0.000454942521173507\n",
      "epoch: 3 step: 375, loss is 0.0007432374986819923\n",
      "epoch: 3 step: 376, loss is 0.000770525773987174\n",
      "epoch: 3 step: 377, loss is 0.001384626142680645\n",
      "epoch: 3 step: 378, loss is 0.001882084645330906\n",
      "epoch: 3 step: 379, loss is 0.0017355402233079076\n",
      "epoch: 3 step: 380, loss is 0.00056086340919137\n",
      "epoch: 3 step: 381, loss is 0.0027681211940944195\n",
      "epoch: 3 step: 382, loss is 0.0012532712426036596\n",
      "epoch: 3 step: 383, loss is 0.0011050938628613949\n",
      "epoch: 3 step: 384, loss is 0.0019222304690629244\n",
      "epoch: 3 step: 385, loss is 0.0005426192656159401\n",
      "epoch: 3 step: 386, loss is 0.000660161895211786\n",
      "epoch: 3 step: 387, loss is 0.0007244052831083536\n",
      "epoch: 3 step: 388, loss is 0.0007499466883018613\n",
      "epoch: 3 step: 389, loss is 0.0007555642514489591\n",
      "epoch: 3 step: 390, loss is 0.001292083878070116\n",
      "epoch: 3 step: 391, loss is 0.0010131103917956352\n",
      "epoch: 3 step: 392, loss is 0.00209721177816391\n",
      "epoch: 3 step: 393, loss is 0.001093172701075673\n",
      "epoch: 3 step: 394, loss is 0.000933973933570087\n",
      "epoch: 3 step: 395, loss is 0.004549939185380936\n",
      "epoch: 3 step: 396, loss is 0.0010210050968453288\n",
      "epoch: 3 step: 397, loss is 0.0008836740162223577\n",
      "epoch: 3 step: 398, loss is 0.0007170089520514011\n",
      "epoch: 3 step: 399, loss is 0.0017295671859756112\n",
      "epoch: 3 step: 400, loss is 0.0015322660328820348\n",
      "epoch: 3 step: 401, loss is 0.0011753856670111418\n",
      "epoch: 3 step: 402, loss is 0.0005724305519834161\n",
      "epoch: 3 step: 403, loss is 0.00698486715555191\n",
      "epoch: 3 step: 404, loss is 0.008012430742383003\n",
      "epoch: 3 step: 405, loss is 0.0018726738635450602\n",
      "epoch: 3 step: 406, loss is 0.0019505682867020369\n",
      "epoch: 3 step: 407, loss is 0.0005049644969403744\n",
      "epoch: 3 step: 408, loss is 0.004326322581619024\n",
      "epoch: 3 step: 409, loss is 0.005589630454778671\n",
      "epoch: 3 step: 410, loss is 0.0006243734387680888\n",
      "epoch: 3 step: 411, loss is 0.0008593915845267475\n",
      "epoch: 3 step: 412, loss is 0.0005357047775760293\n",
      "epoch: 3 step: 413, loss is 0.0017240240704268217\n",
      "epoch: 3 step: 414, loss is 0.0009121174225583673\n",
      "epoch: 3 step: 415, loss is 0.0005978523404337466\n",
      "epoch: 3 step: 416, loss is 0.002701277146115899\n",
      "epoch: 3 step: 417, loss is 0.000590859679505229\n",
      "epoch: 3 step: 418, loss is 0.0004938546335324645\n",
      "epoch: 3 step: 419, loss is 0.00042919666157104075\n",
      "epoch: 3 step: 420, loss is 0.0008838634239509702\n",
      "epoch: 3 step: 421, loss is 0.000619887956418097\n",
      "epoch: 3 step: 422, loss is 0.000557485269382596\n",
      "epoch: 3 step: 423, loss is 0.0006071262760087848\n",
      "epoch: 3 step: 424, loss is 0.002216794528067112\n",
      "epoch: 3 step: 425, loss is 0.0009606676758266985\n",
      "epoch: 3 step: 426, loss is 0.0011392879532650113\n",
      "epoch: 3 step: 427, loss is 0.0005883314879611135\n",
      "epoch: 3 step: 428, loss is 0.0007620135438628495\n",
      "epoch: 3 step: 429, loss is 0.00041079879156313837\n",
      "epoch: 3 step: 430, loss is 0.008798383176326752\n",
      "epoch: 3 step: 431, loss is 0.0012198855401948094\n",
      "epoch: 3 step: 432, loss is 0.002717917552217841\n",
      "epoch: 3 step: 433, loss is 0.0002964820887427777\n",
      "epoch: 3 step: 434, loss is 0.0011447365395724773\n",
      "epoch: 3 step: 435, loss is 0.0022219703532755375\n",
      "epoch: 3 step: 436, loss is 0.0005192532553337514\n",
      "epoch: 3 step: 437, loss is 0.0013606741558760405\n",
      "epoch: 3 step: 438, loss is 0.000816390267573297\n",
      "epoch: 3 step: 439, loss is 0.0011074923677369952\n",
      "epoch: 3 step: 440, loss is 0.0010977969504892826\n",
      "epoch: 3 step: 441, loss is 0.00131172314286232\n",
      "epoch: 3 step: 442, loss is 0.0009015494724735618\n",
      "epoch: 3 step: 443, loss is 0.0008240959141403437\n",
      "epoch: 3 step: 444, loss is 0.001564397243782878\n",
      "epoch: 3 step: 445, loss is 0.0004745679325424135\n",
      "epoch: 3 step: 446, loss is 0.0009101959294639528\n",
      "epoch: 3 step: 447, loss is 0.0003911775420419872\n",
      "epoch: 3 step: 448, loss is 0.00047971386811695993\n",
      "epoch: 3 step: 449, loss is 0.002527971751987934\n",
      "epoch: 3 step: 450, loss is 0.0008318153559230268\n",
      "epoch: 3 step: 451, loss is 0.004479425493627787\n",
      "epoch: 3 step: 452, loss is 0.0012709180591627955\n",
      "epoch: 3 step: 453, loss is 0.0007567794527858496\n",
      "epoch: 3 step: 454, loss is 0.0004555257619358599\n",
      "epoch: 3 step: 455, loss is 0.0004217293462716043\n",
      "epoch: 3 step: 456, loss is 0.001537877251394093\n",
      "epoch: 3 step: 457, loss is 0.002557715866714716\n",
      "epoch: 3 step: 458, loss is 0.004001335706561804\n",
      "epoch: 3 step: 459, loss is 0.0014398599741980433\n",
      "epoch: 3 step: 460, loss is 0.002393634058535099\n",
      "epoch: 3 step: 461, loss is 0.0013592398026958108\n",
      "epoch: 3 step: 462, loss is 0.0005169899668544531\n",
      "epoch: 3 step: 463, loss is 0.0005584215396083891\n",
      "epoch: 3 step: 464, loss is 0.0005328624974936247\n",
      "epoch: 3 step: 465, loss is 0.0037387805059552193\n",
      "epoch: 3 step: 466, loss is 0.0013345463667064905\n",
      "epoch: 3 step: 467, loss is 0.004039098508656025\n",
      "epoch: 3 step: 468, loss is 0.0005323845543898642\n",
      "epoch: 3 step: 469, loss is 0.0007624115678481758\n",
      "epoch: 3 step: 470, loss is 0.001213326584547758\n",
      "epoch: 3 step: 471, loss is 0.001090899109840393\n",
      "epoch: 3 step: 472, loss is 0.0014986024470999837\n",
      "epoch: 3 step: 473, loss is 0.0002503526920918375\n",
      "epoch: 3 step: 474, loss is 0.0028375769034028053\n",
      "epoch: 3 step: 475, loss is 0.0005812454619444907\n",
      "epoch: 3 step: 476, loss is 0.001613386906683445\n",
      "epoch: 3 step: 477, loss is 0.0032204582821577787\n",
      "epoch: 3 step: 478, loss is 0.000816397718153894\n",
      "epoch: 3 step: 479, loss is 0.0007093122694641352\n",
      "epoch: 3 step: 480, loss is 0.002844214206561446\n",
      "epoch: 3 step: 481, loss is 0.0006674717296846211\n",
      "epoch: 3 step: 482, loss is 0.0013580736704170704\n",
      "epoch: 3 step: 483, loss is 0.000946891843341291\n",
      "epoch: 3 step: 484, loss is 0.001282988698221743\n",
      "epoch: 3 step: 485, loss is 0.0006503810291178524\n",
      "epoch: 3 step: 486, loss is 0.00041449785931035876\n",
      "epoch: 3 step: 487, loss is 0.0002949184854514897\n",
      "epoch: 3 step: 488, loss is 0.001619473798200488\n",
      "epoch: 3 step: 489, loss is 0.0013457396999001503\n",
      "epoch: 3 step: 490, loss is 0.00035489967558532953\n",
      "epoch: 3 step: 491, loss is 0.0012907894561067224\n",
      "epoch: 3 step: 492, loss is 0.0007402879418805242\n",
      "epoch: 3 step: 493, loss is 0.0009717487846501172\n",
      "epoch: 3 step: 494, loss is 0.003452220931649208\n",
      "epoch: 3 step: 495, loss is 0.0010380337480455637\n",
      "epoch: 3 step: 496, loss is 0.00093551364261657\n",
      "epoch: 3 step: 497, loss is 0.0018354541389271617\n",
      "epoch: 3 step: 498, loss is 0.000257067964412272\n",
      "epoch: 3 step: 499, loss is 0.001068292767740786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 500, loss is 0.0021120519377291203\n",
      "epoch: 3 step: 501, loss is 0.0013812528923153877\n",
      "epoch: 3 step: 502, loss is 0.0025696721859276295\n",
      "epoch: 3 step: 503, loss is 0.0003185533278156072\n",
      "epoch: 3 step: 504, loss is 0.00035687710624188185\n",
      "epoch: 3 step: 505, loss is 0.0004920833162032068\n",
      "epoch: 3 step: 506, loss is 0.0007473533623851836\n",
      "epoch: 3 step: 507, loss is 0.0014447747962549329\n",
      "epoch: 3 step: 508, loss is 0.000186846504220739\n",
      "epoch: 3 step: 509, loss is 0.0005006248247809708\n",
      "epoch: 3 step: 510, loss is 0.001926638768054545\n",
      "epoch: 3 step: 511, loss is 0.0011603221064433455\n",
      "epoch: 3 step: 512, loss is 0.0016511155990883708\n",
      "epoch: 3 step: 513, loss is 0.000932565308175981\n",
      "epoch: 3 step: 514, loss is 0.0012362822890281677\n",
      "epoch: 3 step: 515, loss is 0.004725092556327581\n",
      "epoch: 3 step: 516, loss is 0.001706699375063181\n",
      "epoch: 3 step: 517, loss is 0.004799210466444492\n",
      "epoch: 3 step: 518, loss is 0.0006164931692183018\n",
      "epoch: 3 step: 519, loss is 0.0012729999143630266\n",
      "epoch: 3 step: 520, loss is 0.0007974097970873117\n",
      "epoch: 3 step: 521, loss is 0.00168547872453928\n",
      "epoch: 3 step: 522, loss is 0.0013702799333259463\n",
      "epoch: 3 step: 523, loss is 0.0006077724974602461\n",
      "epoch: 3 step: 524, loss is 0.0013628380838781595\n",
      "epoch: 3 step: 525, loss is 0.0002968227490782738\n",
      "epoch: 3 step: 526, loss is 0.0009583716164343059\n",
      "epoch: 3 step: 527, loss is 0.0005924987490288913\n",
      "epoch: 3 step: 528, loss is 0.0014162991428747773\n",
      "epoch: 3 step: 529, loss is 0.0013142706593498588\n",
      "epoch: 3 step: 530, loss is 0.00103706493973732\n",
      "epoch: 3 step: 531, loss is 0.00046127100358717144\n",
      "epoch: 3 step: 532, loss is 0.0013256727252155542\n",
      "epoch: 3 step: 533, loss is 0.0009458367130719125\n",
      "epoch: 3 step: 534, loss is 0.0006841466529294848\n",
      "epoch: 3 step: 535, loss is 0.0003592673456296325\n",
      "epoch: 3 step: 536, loss is 0.0009420397691428661\n",
      "epoch: 3 step: 537, loss is 0.0006434570532292128\n",
      "epoch: 3 step: 538, loss is 0.0006102695479057729\n",
      "epoch: 3 step: 539, loss is 0.00034225150011479855\n",
      "epoch: 3 step: 540, loss is 0.0004532185848802328\n",
      "epoch: 3 step: 541, loss is 0.000722626456990838\n",
      "epoch: 3 step: 542, loss is 0.0008433798211626709\n",
      "epoch: 3 step: 543, loss is 0.003111298196017742\n",
      "epoch: 3 step: 544, loss is 0.0015125267673283815\n",
      "epoch: 3 step: 545, loss is 0.0008347429102286696\n",
      "epoch: 3 step: 546, loss is 0.0006411447538994253\n",
      "epoch: 3 step: 547, loss is 0.000502400507684797\n",
      "epoch: 3 step: 548, loss is 0.001972213387489319\n",
      "epoch: 3 step: 549, loss is 0.0013364579062908888\n",
      "epoch: 3 step: 550, loss is 0.0006780122639611363\n",
      "epoch: 3 step: 551, loss is 0.0006894470425322652\n",
      "epoch: 3 step: 552, loss is 0.0006515530403703451\n",
      "epoch: 3 step: 553, loss is 0.007969056256115437\n",
      "epoch: 3 step: 554, loss is 0.0007098905043676496\n",
      "epoch: 3 step: 555, loss is 0.0008209697552956641\n",
      "epoch: 3 step: 556, loss is 0.0008329818956553936\n",
      "epoch: 3 step: 557, loss is 0.001011400599963963\n",
      "epoch: 3 step: 558, loss is 0.0009662003722041845\n",
      "epoch: 3 step: 559, loss is 0.000818887900095433\n",
      "epoch: 3 step: 560, loss is 0.0006648653652518988\n",
      "epoch: 3 step: 561, loss is 0.0008534087100997567\n",
      "epoch: 3 step: 562, loss is 0.0005044155404902995\n",
      "epoch: 3 step: 563, loss is 0.000800524721853435\n",
      "epoch: 3 step: 564, loss is 0.0006206734105944633\n",
      "epoch: 3 step: 565, loss is 0.011903101578354836\n",
      "epoch: 3 step: 566, loss is 0.0003844778402708471\n",
      "epoch: 3 step: 567, loss is 0.0001585176942171529\n",
      "epoch: 3 step: 568, loss is 0.00044756996794603765\n",
      "epoch: 3 step: 569, loss is 0.0004425031365826726\n",
      "epoch: 3 step: 570, loss is 0.00037931607221253216\n",
      "epoch: 3 step: 571, loss is 0.00044188223546370864\n",
      "epoch: 3 step: 572, loss is 0.0002974550880026072\n",
      "epoch: 3 step: 573, loss is 0.0013813087716698647\n",
      "epoch: 3 step: 574, loss is 0.0014778324402868748\n",
      "epoch: 3 step: 575, loss is 0.0007715882966294885\n",
      "epoch: 3 step: 576, loss is 0.000810659839771688\n",
      "epoch: 3 step: 577, loss is 0.0004996662610210478\n",
      "epoch: 3 step: 578, loss is 0.0005059196264483035\n",
      "epoch: 3 step: 579, loss is 0.0011183955939486623\n",
      "epoch: 3 step: 580, loss is 0.003784536151215434\n",
      "epoch: 3 step: 581, loss is 0.0007195943617261946\n",
      "epoch: 3 step: 582, loss is 0.0006601866916753352\n",
      "epoch: 3 step: 583, loss is 0.0011596910189837217\n",
      "epoch: 3 step: 584, loss is 0.0019651236943900585\n",
      "epoch: 3 step: 585, loss is 0.00045860325917601585\n",
      "epoch: 3 step: 586, loss is 0.001005562487989664\n",
      "epoch: 3 step: 587, loss is 0.0027075347024947405\n",
      "epoch: 3 step: 588, loss is 0.0016623407136648893\n",
      "epoch: 3 step: 589, loss is 0.0007743847672827542\n",
      "epoch: 3 step: 590, loss is 0.0008104630978778005\n",
      "epoch: 3 step: 591, loss is 0.0005424711271189153\n",
      "epoch: 3 step: 592, loss is 0.0017899619415402412\n",
      "epoch: 3 step: 593, loss is 0.00148324528709054\n",
      "epoch: 3 step: 594, loss is 0.00047998660011217\n",
      "epoch: 3 step: 595, loss is 0.0010320674628019333\n",
      "epoch: 3 step: 596, loss is 0.0003994617727585137\n",
      "Train epoch time: 31357.165 ms, per step time: 52.613 ms\n",
      "epoch: 4 step: 1, loss is 0.0005126711912453175\n",
      "epoch: 4 step: 2, loss is 0.00041494768811389804\n",
      "epoch: 4 step: 3, loss is 0.00035102744004689157\n",
      "epoch: 4 step: 4, loss is 0.0018331045284867287\n",
      "epoch: 4 step: 5, loss is 0.0017447550781071186\n",
      "epoch: 4 step: 6, loss is 0.0015378050738945603\n",
      "epoch: 4 step: 7, loss is 0.0006061363383196294\n",
      "epoch: 4 step: 8, loss is 0.00027945812325924635\n",
      "epoch: 4 step: 9, loss is 0.0012000296264886856\n",
      "epoch: 4 step: 10, loss is 0.001100543886423111\n",
      "epoch: 4 step: 11, loss is 0.0010899212211370468\n",
      "epoch: 4 step: 12, loss is 0.0018157042795792222\n",
      "epoch: 4 step: 13, loss is 0.0005942852585576475\n",
      "epoch: 4 step: 14, loss is 0.0004795500135514885\n",
      "epoch: 4 step: 15, loss is 0.000843445654027164\n",
      "epoch: 4 step: 16, loss is 0.0005959027912467718\n",
      "epoch: 4 step: 17, loss is 0.0004974431358277798\n",
      "epoch: 4 step: 18, loss is 0.0008113001240417361\n",
      "epoch: 4 step: 19, loss is 0.001037779962643981\n",
      "epoch: 4 step: 20, loss is 0.0010579016525298357\n",
      "epoch: 4 step: 21, loss is 0.0009955255081877112\n",
      "epoch: 4 step: 22, loss is 0.00020060101815033704\n",
      "epoch: 4 step: 23, loss is 0.0006160096381790936\n",
      "epoch: 4 step: 24, loss is 0.0005026371800340712\n",
      "epoch: 4 step: 25, loss is 0.0005774486344307661\n",
      "epoch: 4 step: 26, loss is 0.0006679326761513948\n",
      "epoch: 4 step: 27, loss is 0.00301446788944304\n",
      "epoch: 4 step: 28, loss is 0.0011391150765120983\n",
      "epoch: 4 step: 29, loss is 0.0005973730585537851\n",
      "epoch: 4 step: 30, loss is 0.0029930416494607925\n",
      "epoch: 4 step: 31, loss is 0.0006376910605467856\n",
      "epoch: 4 step: 32, loss is 0.001228976878337562\n",
      "epoch: 4 step: 33, loss is 0.001003776560537517\n",
      "epoch: 4 step: 34, loss is 0.0007009286200627685\n",
      "epoch: 4 step: 35, loss is 0.0010703362058848143\n",
      "epoch: 4 step: 36, loss is 0.0115459393709898\n",
      "epoch: 4 step: 37, loss is 0.0008107314351946115\n",
      "epoch: 4 step: 38, loss is 0.0007383774500340223\n",
      "epoch: 4 step: 39, loss is 0.0005911359912715852\n",
      "epoch: 4 step: 40, loss is 0.0011564994929358363\n",
      "epoch: 4 step: 41, loss is 0.0020111086778342724\n",
      "epoch: 4 step: 42, loss is 0.001640494097955525\n",
      "epoch: 4 step: 43, loss is 0.0002986108884215355\n",
      "epoch: 4 step: 44, loss is 0.0010007154196500778\n",
      "epoch: 4 step: 45, loss is 0.0007713097147643566\n",
      "epoch: 4 step: 46, loss is 0.0005294129950925708\n",
      "epoch: 4 step: 47, loss is 0.001980179687961936\n",
      "epoch: 4 step: 48, loss is 0.00046903890324756503\n",
      "epoch: 4 step: 49, loss is 0.00023178385163191706\n",
      "epoch: 4 step: 50, loss is 0.0007078658672980964\n",
      "epoch: 4 step: 51, loss is 0.0001696137769613415\n",
      "epoch: 4 step: 52, loss is 0.0002677026786841452\n",
      "epoch: 4 step: 53, loss is 0.0020115512888878584\n",
      "epoch: 4 step: 54, loss is 0.0007052534492686391\n",
      "epoch: 4 step: 55, loss is 0.0020756637677550316\n",
      "epoch: 4 step: 56, loss is 0.00047454991727136075\n",
      "epoch: 4 step: 57, loss is 0.0003837837721221149\n",
      "epoch: 4 step: 58, loss is 0.00024346780264750123\n",
      "epoch: 4 step: 59, loss is 0.0005059437244199216\n",
      "epoch: 4 step: 60, loss is 0.0003121835470665246\n",
      "epoch: 4 step: 61, loss is 0.00026362674543634057\n",
      "epoch: 4 step: 62, loss is 0.000361843645805493\n",
      "epoch: 4 step: 63, loss is 0.0001752322568790987\n",
      "epoch: 4 step: 64, loss is 0.0008835624321363866\n",
      "epoch: 4 step: 65, loss is 0.0007189358002506196\n",
      "epoch: 4 step: 66, loss is 0.000384573737392202\n",
      "epoch: 4 step: 67, loss is 0.001547425054013729\n",
      "epoch: 4 step: 68, loss is 0.0011508428724482656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 69, loss is 0.0023381528444588184\n",
      "epoch: 4 step: 70, loss is 0.00037186089321039617\n",
      "epoch: 4 step: 71, loss is 0.0005707233212888241\n",
      "epoch: 4 step: 72, loss is 0.0016999756917357445\n",
      "epoch: 4 step: 73, loss is 0.00017220544395968318\n",
      "epoch: 4 step: 74, loss is 0.0008891057223081589\n",
      "epoch: 4 step: 75, loss is 0.0007738709100522101\n",
      "epoch: 4 step: 76, loss is 0.0002724655787460506\n",
      "epoch: 4 step: 77, loss is 0.0007066407706588507\n",
      "epoch: 4 step: 78, loss is 0.0005547570763155818\n",
      "epoch: 4 step: 79, loss is 0.0007810319075360894\n",
      "epoch: 4 step: 80, loss is 0.0005122203147038817\n",
      "epoch: 4 step: 81, loss is 0.0011805221438407898\n",
      "epoch: 4 step: 82, loss is 0.00045754003804177046\n",
      "epoch: 4 step: 83, loss is 0.0013748318888247013\n",
      "epoch: 4 step: 84, loss is 0.000452680338639766\n",
      "epoch: 4 step: 85, loss is 0.0005928942118771374\n",
      "epoch: 4 step: 86, loss is 0.0007122340612113476\n",
      "epoch: 4 step: 87, loss is 0.00021820818074047565\n",
      "epoch: 4 step: 88, loss is 0.000401204772060737\n",
      "epoch: 4 step: 89, loss is 0.0007168307201936841\n",
      "epoch: 4 step: 90, loss is 0.0008022356196306646\n",
      "epoch: 4 step: 91, loss is 0.0006124239880591631\n",
      "epoch: 4 step: 92, loss is 0.00021245448442641646\n",
      "epoch: 4 step: 93, loss is 0.0007840542239136994\n",
      "epoch: 4 step: 94, loss is 0.009191462770104408\n",
      "epoch: 4 step: 95, loss is 0.0006094701820984483\n",
      "epoch: 4 step: 96, loss is 0.0007143240072764456\n",
      "epoch: 4 step: 97, loss is 0.0011251467512920499\n",
      "epoch: 4 step: 98, loss is 0.0004094785253982991\n",
      "epoch: 4 step: 99, loss is 0.0034608461428433657\n",
      "epoch: 4 step: 100, loss is 0.0007484751404263079\n",
      "epoch: 4 step: 101, loss is 0.008659008890390396\n",
      "epoch: 4 step: 102, loss is 0.0010690223425626755\n",
      "epoch: 4 step: 103, loss is 0.0009624727535992861\n",
      "epoch: 4 step: 104, loss is 0.0003579629410523921\n",
      "epoch: 4 step: 105, loss is 0.0005282514612190425\n",
      "epoch: 4 step: 106, loss is 0.0048236241564154625\n",
      "epoch: 4 step: 107, loss is 0.0006103628547862172\n",
      "epoch: 4 step: 108, loss is 0.00038343080086633563\n",
      "epoch: 4 step: 109, loss is 0.0007058848859742284\n",
      "epoch: 4 step: 110, loss is 0.0016540518263354897\n",
      "epoch: 4 step: 111, loss is 0.0014213693793863058\n",
      "epoch: 4 step: 112, loss is 0.0010768505744636059\n",
      "epoch: 4 step: 113, loss is 0.0011048458982259035\n",
      "epoch: 4 step: 114, loss is 0.0005159717984497547\n",
      "epoch: 4 step: 115, loss is 0.0004959439393132925\n",
      "epoch: 4 step: 116, loss is 0.0006492898100987077\n",
      "epoch: 4 step: 117, loss is 0.000386827508918941\n",
      "epoch: 4 step: 118, loss is 0.0009080930612981319\n",
      "epoch: 4 step: 119, loss is 0.0007852136041037738\n",
      "epoch: 4 step: 120, loss is 0.00032291951356455684\n",
      "epoch: 4 step: 121, loss is 0.0007371879764832556\n",
      "epoch: 4 step: 122, loss is 0.0020545832812786102\n",
      "epoch: 4 step: 123, loss is 0.00024454775848425925\n",
      "epoch: 4 step: 124, loss is 0.00018065175390802324\n",
      "epoch: 4 step: 125, loss is 0.00035804446088150144\n",
      "epoch: 4 step: 126, loss is 0.0007979327347129583\n",
      "epoch: 4 step: 127, loss is 0.00025534993619658053\n",
      "epoch: 4 step: 128, loss is 0.0015264700632542372\n",
      "epoch: 4 step: 129, loss is 0.0003273975453339517\n",
      "epoch: 4 step: 130, loss is 0.0003966597141698003\n",
      "epoch: 4 step: 131, loss is 0.0004857717140112072\n",
      "epoch: 4 step: 132, loss is 0.0005113811930641532\n",
      "epoch: 4 step: 133, loss is 0.0006402141880244017\n",
      "epoch: 4 step: 134, loss is 0.001316799665801227\n",
      "epoch: 4 step: 135, loss is 0.00027700053760781884\n",
      "epoch: 4 step: 136, loss is 0.0005857401993125677\n",
      "epoch: 4 step: 137, loss is 0.0020074243657290936\n",
      "epoch: 4 step: 138, loss is 0.00039419005042873323\n",
      "epoch: 4 step: 139, loss is 0.0012135740835219622\n",
      "epoch: 4 step: 140, loss is 0.0005361804505810142\n",
      "epoch: 4 step: 141, loss is 0.0007430248078890145\n",
      "epoch: 4 step: 142, loss is 0.0006126549560576677\n",
      "epoch: 4 step: 143, loss is 0.0007061262149363756\n",
      "epoch: 4 step: 144, loss is 0.00036632129922509193\n",
      "epoch: 4 step: 145, loss is 0.009957840666174889\n",
      "epoch: 4 step: 146, loss is 0.001125134527683258\n",
      "epoch: 4 step: 147, loss is 0.0002872696495614946\n",
      "epoch: 4 step: 148, loss is 0.001001297845505178\n",
      "epoch: 4 step: 149, loss is 0.000817903142888099\n",
      "epoch: 4 step: 150, loss is 0.0005004270933568478\n",
      "epoch: 4 step: 151, loss is 0.00030655512819066644\n",
      "epoch: 4 step: 152, loss is 0.0006773592904210091\n",
      "epoch: 4 step: 153, loss is 0.0016122907400131226\n",
      "epoch: 4 step: 154, loss is 0.0010243947617709637\n",
      "epoch: 4 step: 155, loss is 0.0010947393020614982\n",
      "epoch: 4 step: 156, loss is 0.0005714838043786585\n",
      "epoch: 4 step: 157, loss is 0.0005451265024021268\n",
      "epoch: 4 step: 158, loss is 0.0010196622461080551\n",
      "epoch: 4 step: 159, loss is 0.0012790897162631154\n",
      "epoch: 4 step: 160, loss is 0.0012611992424353957\n",
      "epoch: 4 step: 161, loss is 0.0004943731473758817\n",
      "epoch: 4 step: 162, loss is 0.0008181879529729486\n",
      "epoch: 4 step: 163, loss is 0.0007839856552891433\n",
      "epoch: 4 step: 164, loss is 0.00033108502975665033\n",
      "epoch: 4 step: 165, loss is 0.00037219590740278363\n",
      "epoch: 4 step: 166, loss is 0.0010264958254992962\n",
      "epoch: 4 step: 167, loss is 0.0008517628302797675\n",
      "epoch: 4 step: 168, loss is 0.0008586414041928947\n",
      "epoch: 4 step: 169, loss is 0.0010843612253665924\n",
      "epoch: 4 step: 170, loss is 0.0004982845857739449\n",
      "epoch: 4 step: 171, loss is 0.00037822406738996506\n",
      "epoch: 4 step: 172, loss is 0.001372055266983807\n",
      "epoch: 4 step: 173, loss is 0.00025034157442860305\n",
      "epoch: 4 step: 174, loss is 0.00021531952370423824\n",
      "epoch: 4 step: 175, loss is 0.0006921045714989305\n",
      "epoch: 4 step: 176, loss is 0.002008182927966118\n",
      "epoch: 4 step: 177, loss is 0.001603206037543714\n",
      "epoch: 4 step: 178, loss is 0.001043071853928268\n",
      "epoch: 4 step: 179, loss is 0.0018497926648706198\n",
      "epoch: 4 step: 180, loss is 0.001220437465235591\n",
      "epoch: 4 step: 181, loss is 0.000514938379637897\n",
      "epoch: 4 step: 182, loss is 0.0009342771372757852\n",
      "epoch: 4 step: 183, loss is 0.0002656793803907931\n",
      "epoch: 4 step: 184, loss is 0.0009995348518714309\n",
      "epoch: 4 step: 185, loss is 0.002807898446917534\n",
      "epoch: 4 step: 186, loss is 0.0011304146610200405\n",
      "epoch: 4 step: 187, loss is 0.004470794927328825\n",
      "epoch: 4 step: 188, loss is 0.00043881681631319225\n",
      "epoch: 4 step: 189, loss is 0.0005065593868494034\n",
      "epoch: 4 step: 190, loss is 0.0017439539078623056\n",
      "epoch: 4 step: 191, loss is 0.0019092133734375238\n",
      "epoch: 4 step: 192, loss is 0.0003989769902545959\n",
      "epoch: 4 step: 193, loss is 0.000976470997557044\n",
      "epoch: 4 step: 194, loss is 0.0004012960707768798\n",
      "epoch: 4 step: 195, loss is 0.0006115806172601879\n",
      "epoch: 4 step: 196, loss is 0.0024593586567789316\n",
      "epoch: 4 step: 197, loss is 0.0013060711789876223\n",
      "epoch: 4 step: 198, loss is 0.00031075754668563604\n",
      "epoch: 4 step: 199, loss is 0.0009352249326184392\n",
      "epoch: 4 step: 200, loss is 0.00020807985856663436\n",
      "epoch: 4 step: 201, loss is 0.0001742157037369907\n",
      "epoch: 4 step: 202, loss is 0.0006796364323236048\n",
      "epoch: 4 step: 203, loss is 0.0006083255866542459\n",
      "epoch: 4 step: 204, loss is 0.0015974217094480991\n",
      "epoch: 4 step: 205, loss is 0.0002794249448925257\n",
      "epoch: 4 step: 206, loss is 0.0003349360194988549\n",
      "epoch: 4 step: 207, loss is 0.0005299331387504935\n",
      "epoch: 4 step: 208, loss is 0.0004227256868034601\n",
      "epoch: 4 step: 209, loss is 0.00306134601123631\n",
      "epoch: 4 step: 210, loss is 0.00014910398749634624\n",
      "epoch: 4 step: 211, loss is 0.00042776737245731056\n",
      "epoch: 4 step: 212, loss is 0.0001513498864369467\n",
      "epoch: 4 step: 213, loss is 0.000716772919986397\n",
      "epoch: 4 step: 214, loss is 0.0010975552722811699\n",
      "epoch: 4 step: 215, loss is 0.0005417178617790341\n",
      "epoch: 4 step: 216, loss is 0.002983047626912594\n",
      "epoch: 4 step: 217, loss is 0.0010618600063025951\n",
      "epoch: 4 step: 218, loss is 0.0011002486571669579\n",
      "epoch: 4 step: 219, loss is 0.0006832954240962863\n",
      "epoch: 4 step: 220, loss is 0.0004529246944002807\n",
      "epoch: 4 step: 221, loss is 0.001241805381141603\n",
      "epoch: 4 step: 222, loss is 0.00021954630210530013\n",
      "epoch: 4 step: 223, loss is 0.0008125395979732275\n",
      "epoch: 4 step: 224, loss is 0.0006729853339493275\n",
      "epoch: 4 step: 225, loss is 0.00022898618772160262\n",
      "epoch: 4 step: 226, loss is 0.001689710421487689\n",
      "epoch: 4 step: 227, loss is 0.005892113316804171\n",
      "epoch: 4 step: 228, loss is 0.00025964091764763\n",
      "epoch: 4 step: 229, loss is 0.000856467173434794\n",
      "epoch: 4 step: 230, loss is 0.00135163520462811\n",
      "epoch: 4 step: 231, loss is 0.0005147330812178552\n",
      "epoch: 4 step: 232, loss is 0.00035878061316907406\n",
      "epoch: 4 step: 233, loss is 0.0005999634740874171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 234, loss is 0.0010058479383587837\n",
      "epoch: 4 step: 235, loss is 0.000656931078992784\n",
      "epoch: 4 step: 236, loss is 0.0025092207361012697\n",
      "epoch: 4 step: 237, loss is 0.0006589965196326375\n",
      "epoch: 4 step: 238, loss is 0.0005778519553132355\n",
      "epoch: 4 step: 239, loss is 0.0010367401409894228\n",
      "epoch: 4 step: 240, loss is 0.0007606750587001443\n",
      "epoch: 4 step: 241, loss is 0.000400731572881341\n",
      "epoch: 4 step: 242, loss is 0.00647029047831893\n",
      "epoch: 4 step: 243, loss is 0.0005277217132970691\n",
      "epoch: 4 step: 244, loss is 0.000397468451410532\n",
      "epoch: 4 step: 245, loss is 0.0002913043717853725\n",
      "epoch: 4 step: 246, loss is 0.0006049717194400728\n",
      "epoch: 4 step: 247, loss is 0.00020741153275594115\n",
      "epoch: 4 step: 248, loss is 0.0004051247669849545\n",
      "epoch: 4 step: 249, loss is 0.0008120929123833776\n",
      "epoch: 4 step: 250, loss is 0.001063306350260973\n",
      "epoch: 4 step: 251, loss is 0.0008193391840904951\n",
      "epoch: 4 step: 252, loss is 0.0017520993715152144\n",
      "epoch: 4 step: 253, loss is 0.0015182264614850283\n",
      "epoch: 4 step: 254, loss is 0.0028095445595681667\n",
      "epoch: 4 step: 255, loss is 0.004783960524946451\n",
      "epoch: 4 step: 256, loss is 0.028872432187199593\n",
      "epoch: 4 step: 257, loss is 0.0005931082414463162\n",
      "epoch: 4 step: 258, loss is 0.0003315467038191855\n",
      "epoch: 4 step: 259, loss is 0.00044860399793833494\n",
      "epoch: 4 step: 260, loss is 0.0007798033766448498\n",
      "epoch: 4 step: 261, loss is 0.0007027412066236138\n",
      "epoch: 4 step: 262, loss is 0.00038783124182373285\n",
      "epoch: 4 step: 263, loss is 0.000876318197697401\n",
      "epoch: 4 step: 264, loss is 0.0002888563903979957\n",
      "epoch: 4 step: 265, loss is 0.0005406850250437856\n",
      "epoch: 4 step: 266, loss is 0.0005290174158290029\n",
      "epoch: 4 step: 267, loss is 0.0010200550314038992\n",
      "epoch: 4 step: 268, loss is 0.0005711145349778235\n",
      "epoch: 4 step: 269, loss is 0.000170865620020777\n",
      "epoch: 4 step: 270, loss is 0.0002588334900792688\n",
      "epoch: 4 step: 271, loss is 0.002514304593205452\n",
      "epoch: 4 step: 272, loss is 0.00018683963571675122\n",
      "epoch: 4 step: 273, loss is 0.00024941336596384645\n",
      "epoch: 4 step: 274, loss is 0.00022242922568693757\n",
      "epoch: 4 step: 275, loss is 0.00032011608709581196\n",
      "epoch: 4 step: 276, loss is 0.0003479183360468596\n",
      "epoch: 4 step: 277, loss is 0.00035691424272954464\n",
      "epoch: 4 step: 278, loss is 0.0008437882643193007\n",
      "epoch: 4 step: 279, loss is 0.000438191054854542\n",
      "epoch: 4 step: 280, loss is 0.0005397065542638302\n",
      "epoch: 4 step: 281, loss is 0.00035700987791642547\n",
      "epoch: 4 step: 282, loss is 0.0003508469962980598\n",
      "epoch: 4 step: 283, loss is 0.001312548527494073\n",
      "epoch: 4 step: 284, loss is 0.0003031479427590966\n",
      "epoch: 4 step: 285, loss is 0.0005535087548196316\n",
      "epoch: 4 step: 286, loss is 0.0018285978585481644\n",
      "epoch: 4 step: 287, loss is 0.00037980740307830274\n",
      "epoch: 4 step: 288, loss is 0.0005706545198336244\n",
      "epoch: 4 step: 289, loss is 0.00044001571950502694\n",
      "epoch: 4 step: 290, loss is 0.0021458398550748825\n",
      "epoch: 4 step: 291, loss is 0.0005249660462141037\n",
      "epoch: 4 step: 292, loss is 0.0014050077879801393\n",
      "epoch: 4 step: 293, loss is 0.00040175538742914796\n",
      "epoch: 4 step: 294, loss is 0.00047379074385389686\n",
      "epoch: 4 step: 295, loss is 0.0012168808607384562\n",
      "epoch: 4 step: 296, loss is 0.0002372034068685025\n",
      "epoch: 4 step: 297, loss is 0.00044070312287658453\n",
      "epoch: 4 step: 298, loss is 0.0013222089037299156\n",
      "epoch: 4 step: 299, loss is 0.0007429882534779608\n",
      "epoch: 4 step: 300, loss is 0.0004155177157372236\n",
      "epoch: 4 step: 301, loss is 0.00030603245249949396\n",
      "epoch: 4 step: 302, loss is 0.0027394809294492006\n",
      "epoch: 4 step: 303, loss is 0.0017788672121241689\n",
      "epoch: 4 step: 304, loss is 0.0007840715115889907\n",
      "epoch: 4 step: 305, loss is 0.00046133087016642094\n",
      "epoch: 4 step: 306, loss is 0.0002476594236213714\n",
      "epoch: 4 step: 307, loss is 0.001193763455376029\n",
      "epoch: 4 step: 308, loss is 0.0032288683578372\n",
      "epoch: 4 step: 309, loss is 0.0008273085113614798\n",
      "epoch: 4 step: 310, loss is 0.001448431401513517\n",
      "epoch: 4 step: 311, loss is 0.0006791491759940982\n",
      "epoch: 4 step: 312, loss is 0.0005643524928018451\n",
      "epoch: 4 step: 313, loss is 0.0017264506313949823\n",
      "epoch: 4 step: 314, loss is 0.00023993296781554818\n",
      "epoch: 4 step: 315, loss is 0.0005457169027067721\n",
      "epoch: 4 step: 316, loss is 0.0007476131431758404\n",
      "epoch: 4 step: 317, loss is 0.000700152013450861\n",
      "epoch: 4 step: 318, loss is 0.0016698719700798392\n",
      "epoch: 4 step: 319, loss is 0.00024473658413626254\n",
      "epoch: 4 step: 320, loss is 0.0006958223530091345\n",
      "epoch: 4 step: 321, loss is 0.0002430171298328787\n",
      "epoch: 4 step: 322, loss is 0.000254415295785293\n",
      "epoch: 4 step: 323, loss is 0.0028258035890758038\n",
      "epoch: 4 step: 324, loss is 0.00035935972118750215\n",
      "epoch: 4 step: 325, loss is 0.0036468305625021458\n",
      "epoch: 4 step: 326, loss is 0.0009633314330130816\n",
      "epoch: 4 step: 327, loss is 0.00040089478716254234\n",
      "epoch: 4 step: 328, loss is 0.0017974928487092257\n",
      "epoch: 4 step: 329, loss is 0.00036417736555449665\n",
      "epoch: 4 step: 330, loss is 0.0008495704969391227\n",
      "epoch: 4 step: 331, loss is 0.0007840553298592567\n",
      "epoch: 4 step: 332, loss is 0.00120974937453866\n",
      "epoch: 4 step: 333, loss is 0.0013056729221716523\n",
      "epoch: 4 step: 334, loss is 0.0017833802849054337\n",
      "epoch: 4 step: 335, loss is 0.0007241866551339626\n",
      "epoch: 4 step: 336, loss is 0.0006506115896627307\n",
      "epoch: 4 step: 337, loss is 0.001095438376069069\n",
      "epoch: 4 step: 338, loss is 0.0005207710200920701\n",
      "epoch: 4 step: 339, loss is 0.0008008633158169687\n",
      "epoch: 4 step: 340, loss is 0.0011252411641180515\n",
      "epoch: 4 step: 341, loss is 0.0005944555159658194\n",
      "epoch: 4 step: 342, loss is 0.0009005097672343254\n",
      "epoch: 4 step: 343, loss is 0.0006904560141265392\n",
      "epoch: 4 step: 344, loss is 0.00031780166318640113\n",
      "epoch: 4 step: 345, loss is 0.0028172850143164396\n",
      "epoch: 4 step: 346, loss is 0.0005854908376932144\n",
      "epoch: 4 step: 347, loss is 0.000412207271438092\n",
      "epoch: 4 step: 348, loss is 0.0006462665041908622\n",
      "epoch: 4 step: 349, loss is 0.00048192316899076104\n",
      "epoch: 4 step: 350, loss is 0.00018703403475228697\n",
      "epoch: 4 step: 351, loss is 0.0012895548716187477\n",
      "epoch: 4 step: 352, loss is 0.000587643007747829\n",
      "epoch: 4 step: 353, loss is 0.0012960233725607395\n",
      "epoch: 4 step: 354, loss is 0.0003096260770689696\n",
      "epoch: 4 step: 355, loss is 0.00037275097565725446\n",
      "epoch: 4 step: 356, loss is 0.00015041492588352412\n",
      "epoch: 4 step: 357, loss is 0.0007457426399923861\n",
      "epoch: 4 step: 358, loss is 0.0005306206876412034\n",
      "epoch: 4 step: 359, loss is 0.00015570160758215934\n",
      "epoch: 4 step: 360, loss is 0.0011885258136317134\n",
      "epoch: 4 step: 361, loss is 0.0002860386739484966\n",
      "epoch: 4 step: 362, loss is 0.0006536960136145353\n",
      "epoch: 4 step: 363, loss is 0.001189792761579156\n",
      "epoch: 4 step: 364, loss is 0.001186038600280881\n",
      "epoch: 4 step: 365, loss is 0.0024486323818564415\n",
      "epoch: 4 step: 366, loss is 0.004128944594413042\n",
      "epoch: 4 step: 367, loss is 0.00156774849165231\n",
      "epoch: 4 step: 368, loss is 0.0004411897389218211\n",
      "epoch: 4 step: 369, loss is 0.00034308736212551594\n",
      "epoch: 4 step: 370, loss is 0.0014197679702192545\n",
      "epoch: 4 step: 371, loss is 0.00032551720505580306\n",
      "epoch: 4 step: 372, loss is 0.000907019479200244\n",
      "epoch: 4 step: 373, loss is 0.000773080566432327\n",
      "epoch: 4 step: 374, loss is 0.00032439015922136605\n",
      "epoch: 4 step: 375, loss is 0.0005431945901364088\n",
      "epoch: 4 step: 376, loss is 0.00039688829565420747\n",
      "epoch: 4 step: 377, loss is 0.0006353005301207304\n",
      "epoch: 4 step: 378, loss is 0.000461893854662776\n",
      "epoch: 4 step: 379, loss is 0.0006955282296985388\n",
      "epoch: 4 step: 380, loss is 0.0005121975555084646\n",
      "epoch: 4 step: 381, loss is 0.00041031543514691293\n",
      "epoch: 4 step: 382, loss is 0.0009122461779043078\n",
      "epoch: 4 step: 383, loss is 0.001030861516483128\n",
      "epoch: 4 step: 384, loss is 0.0006674678879790008\n",
      "epoch: 4 step: 385, loss is 0.0006630929419770837\n",
      "epoch: 4 step: 386, loss is 0.0006203891825862229\n",
      "epoch: 4 step: 387, loss is 0.00040776183595880866\n",
      "epoch: 4 step: 388, loss is 0.0008246955694630742\n",
      "epoch: 4 step: 389, loss is 0.0016918935580179095\n",
      "epoch: 4 step: 390, loss is 0.000301560532534495\n",
      "epoch: 4 step: 391, loss is 0.0005554237286560237\n",
      "epoch: 4 step: 392, loss is 0.0017438981449231505\n",
      "epoch: 4 step: 393, loss is 0.0007003256469033659\n",
      "epoch: 4 step: 394, loss is 0.00022227771114557981\n",
      "epoch: 4 step: 395, loss is 0.0007329098298214376\n",
      "epoch: 4 step: 396, loss is 0.00029729216475971043\n",
      "epoch: 4 step: 397, loss is 0.0010496200993657112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 398, loss is 0.0007388044614344835\n",
      "epoch: 4 step: 399, loss is 0.0011332560097798705\n",
      "epoch: 4 step: 400, loss is 0.0007275743409991264\n",
      "epoch: 4 step: 401, loss is 0.0011585548054426908\n",
      "epoch: 4 step: 402, loss is 0.0003401902795303613\n",
      "epoch: 4 step: 403, loss is 0.00033393417834304273\n",
      "epoch: 4 step: 404, loss is 0.004895361606031656\n",
      "epoch: 4 step: 405, loss is 0.00027685397071763873\n",
      "epoch: 4 step: 406, loss is 0.0005003921687602997\n",
      "epoch: 4 step: 407, loss is 0.00031854771077632904\n",
      "epoch: 4 step: 408, loss is 0.0005527150351554155\n",
      "epoch: 4 step: 409, loss is 0.0007160268723964691\n",
      "epoch: 4 step: 410, loss is 0.0003971375699620694\n",
      "epoch: 4 step: 411, loss is 0.0004770146042574197\n",
      "epoch: 4 step: 412, loss is 0.0005674067069776356\n",
      "epoch: 4 step: 413, loss is 0.0010431731352582574\n",
      "epoch: 4 step: 414, loss is 0.0003809751069638878\n",
      "epoch: 4 step: 415, loss is 0.0002487364108674228\n",
      "epoch: 4 step: 416, loss is 0.0008513011271134019\n",
      "epoch: 4 step: 417, loss is 0.0007445281371474266\n",
      "epoch: 4 step: 418, loss is 0.00022863521007820964\n",
      "epoch: 4 step: 419, loss is 0.0003358025860507041\n",
      "epoch: 4 step: 420, loss is 0.0003918721922673285\n",
      "epoch: 4 step: 421, loss is 0.0004419032484292984\n",
      "epoch: 4 step: 422, loss is 0.00018136281869374216\n",
      "epoch: 4 step: 423, loss is 0.00027289579156786203\n",
      "epoch: 4 step: 424, loss is 0.0017875003395602107\n",
      "epoch: 4 step: 425, loss is 0.0006965627544559538\n",
      "epoch: 4 step: 426, loss is 0.0009519036393612623\n",
      "epoch: 4 step: 427, loss is 0.0012314233463257551\n",
      "epoch: 4 step: 428, loss is 0.013708788901567459\n",
      "epoch: 4 step: 429, loss is 0.001491412753239274\n",
      "epoch: 4 step: 430, loss is 0.0016062718350440264\n",
      "epoch: 4 step: 431, loss is 0.0003165135276503861\n",
      "epoch: 4 step: 432, loss is 0.0010317085543647408\n",
      "epoch: 4 step: 433, loss is 0.0003306473954580724\n",
      "epoch: 4 step: 434, loss is 0.0007505432004109025\n",
      "epoch: 4 step: 435, loss is 0.00374725554138422\n",
      "epoch: 4 step: 436, loss is 0.0003764771099667996\n",
      "epoch: 4 step: 437, loss is 0.0005894705536775291\n",
      "epoch: 4 step: 438, loss is 0.0006988188251852989\n",
      "epoch: 4 step: 439, loss is 0.0010136645287275314\n",
      "epoch: 4 step: 440, loss is 0.0009007396874949336\n",
      "epoch: 4 step: 441, loss is 0.0004522069066297263\n",
      "epoch: 4 step: 442, loss is 0.0008713119314052165\n",
      "epoch: 4 step: 443, loss is 0.00042897160165011883\n",
      "epoch: 4 step: 444, loss is 0.0022023539058864117\n",
      "epoch: 4 step: 445, loss is 0.0006054296973161399\n",
      "epoch: 4 step: 446, loss is 0.0005243885098025203\n",
      "epoch: 4 step: 447, loss is 0.0006455983384512365\n",
      "epoch: 4 step: 448, loss is 0.000578060862608254\n",
      "epoch: 4 step: 449, loss is 0.0003512876573950052\n",
      "epoch: 4 step: 450, loss is 0.0001337021531071514\n",
      "epoch: 4 step: 451, loss is 0.0022392868995666504\n",
      "epoch: 4 step: 452, loss is 0.001116062281653285\n",
      "epoch: 4 step: 453, loss is 0.0006102006882429123\n",
      "epoch: 4 step: 454, loss is 0.0003425628237891942\n",
      "epoch: 4 step: 455, loss is 0.0003414481761865318\n",
      "epoch: 4 step: 456, loss is 0.0017366590909659863\n",
      "epoch: 4 step: 457, loss is 0.0008182130986824632\n",
      "epoch: 4 step: 458, loss is 0.001114777522161603\n",
      "epoch: 4 step: 459, loss is 0.0006633533630520105\n",
      "epoch: 4 step: 460, loss is 0.0008922547567635775\n",
      "epoch: 4 step: 461, loss is 0.0007761268643662333\n",
      "epoch: 4 step: 462, loss is 0.0019020489417016506\n",
      "epoch: 4 step: 463, loss is 0.0004293833626434207\n",
      "epoch: 4 step: 464, loss is 0.0004666366148740053\n",
      "epoch: 4 step: 465, loss is 0.0013513516169041395\n",
      "epoch: 4 step: 466, loss is 0.0009337717201560736\n",
      "epoch: 4 step: 467, loss is 0.0011339352931827307\n",
      "epoch: 4 step: 468, loss is 0.0007765513146296144\n",
      "epoch: 4 step: 469, loss is 0.0002086575550492853\n",
      "epoch: 4 step: 470, loss is 0.0010265137534588575\n",
      "epoch: 4 step: 471, loss is 0.0006179736228659749\n",
      "epoch: 4 step: 472, loss is 0.0003003754827659577\n",
      "epoch: 4 step: 473, loss is 0.00039630569517612457\n",
      "epoch: 4 step: 474, loss is 0.0019217171939089894\n",
      "epoch: 4 step: 475, loss is 0.0005347324186004698\n",
      "epoch: 4 step: 476, loss is 0.000629347050562501\n",
      "epoch: 4 step: 477, loss is 0.004229987971484661\n",
      "epoch: 4 step: 478, loss is 0.0014167176559567451\n",
      "epoch: 4 step: 479, loss is 0.0016640342073515058\n",
      "epoch: 4 step: 480, loss is 0.0019712632056325674\n",
      "epoch: 4 step: 481, loss is 0.0011728263925760984\n",
      "epoch: 4 step: 482, loss is 0.001419018255546689\n",
      "epoch: 4 step: 483, loss is 0.002077782992273569\n",
      "epoch: 4 step: 484, loss is 0.0009298358345404267\n",
      "epoch: 4 step: 485, loss is 0.0006449680658988655\n",
      "epoch: 4 step: 486, loss is 0.0005627418286167085\n",
      "epoch: 4 step: 487, loss is 0.0006677353521808982\n",
      "epoch: 4 step: 488, loss is 0.0010746396146714687\n",
      "epoch: 4 step: 489, loss is 0.0008465947466902435\n",
      "epoch: 4 step: 490, loss is 0.0003465458285063505\n",
      "epoch: 4 step: 491, loss is 0.0007685723248869181\n",
      "epoch: 4 step: 492, loss is 0.0007151978788897395\n",
      "epoch: 4 step: 493, loss is 0.00025448028463870287\n",
      "epoch: 4 step: 494, loss is 0.0014473284827545285\n",
      "epoch: 4 step: 495, loss is 0.0005453511839732528\n",
      "epoch: 4 step: 496, loss is 0.0002378700446570292\n",
      "epoch: 4 step: 497, loss is 0.0007802288164384663\n",
      "epoch: 4 step: 498, loss is 0.0003896444395650178\n",
      "epoch: 4 step: 499, loss is 0.00015078717842698097\n",
      "epoch: 4 step: 500, loss is 0.003046887693926692\n",
      "epoch: 4 step: 501, loss is 0.0007746490882709622\n",
      "epoch: 4 step: 502, loss is 0.00266229803673923\n",
      "epoch: 4 step: 503, loss is 0.0005791567964479327\n",
      "epoch: 4 step: 504, loss is 0.00037637416971847415\n",
      "epoch: 4 step: 505, loss is 0.0009816999081522226\n",
      "epoch: 4 step: 506, loss is 0.00037908420199528337\n",
      "epoch: 4 step: 507, loss is 0.0007250843336805701\n",
      "epoch: 4 step: 508, loss is 0.00020973096252419055\n",
      "epoch: 4 step: 509, loss is 0.0005336595932021737\n",
      "epoch: 4 step: 510, loss is 0.00015734252519905567\n",
      "epoch: 4 step: 511, loss is 0.0017156167887151241\n",
      "epoch: 4 step: 512, loss is 0.0007727299816906452\n",
      "epoch: 4 step: 513, loss is 0.0008947853930294514\n",
      "epoch: 4 step: 514, loss is 0.0014371378347277641\n",
      "epoch: 4 step: 515, loss is 0.00043035775888711214\n",
      "epoch: 4 step: 516, loss is 0.0028823367320001125\n",
      "epoch: 4 step: 517, loss is 0.00044041447108611465\n",
      "epoch: 4 step: 518, loss is 0.0006177704781293869\n",
      "epoch: 4 step: 519, loss is 0.0007867926615290344\n",
      "epoch: 4 step: 520, loss is 0.00018190013361163437\n",
      "epoch: 4 step: 521, loss is 0.0005380877992138267\n",
      "epoch: 4 step: 522, loss is 0.0009086428908631206\n",
      "epoch: 4 step: 523, loss is 0.0004516041080933064\n",
      "epoch: 4 step: 524, loss is 0.00036511485814116895\n",
      "epoch: 4 step: 525, loss is 0.0007862198399379849\n",
      "epoch: 4 step: 526, loss is 0.00029395095771178603\n",
      "epoch: 4 step: 527, loss is 0.000578717328608036\n",
      "epoch: 4 step: 528, loss is 0.001347819110378623\n",
      "epoch: 4 step: 529, loss is 0.0007140575908124447\n",
      "epoch: 4 step: 530, loss is 0.00037535131559707224\n",
      "epoch: 4 step: 531, loss is 0.0006427636253647506\n",
      "epoch: 4 step: 532, loss is 0.0003434577083680779\n",
      "epoch: 4 step: 533, loss is 0.001398833584971726\n",
      "epoch: 4 step: 534, loss is 0.001117472886107862\n",
      "epoch: 4 step: 535, loss is 0.0006396931712515652\n",
      "epoch: 4 step: 536, loss is 0.0003472739481367171\n",
      "epoch: 4 step: 537, loss is 0.000580659369006753\n",
      "epoch: 4 step: 538, loss is 0.0005686383228749037\n",
      "epoch: 4 step: 539, loss is 0.0005025822902098298\n",
      "epoch: 4 step: 540, loss is 0.001900934730656445\n",
      "epoch: 4 step: 541, loss is 0.0013705879682675004\n",
      "epoch: 4 step: 542, loss is 0.0006744640995748341\n",
      "epoch: 4 step: 543, loss is 0.00012704684922937304\n",
      "epoch: 4 step: 544, loss is 0.00023952215269673616\n",
      "epoch: 4 step: 545, loss is 0.0003036272246390581\n",
      "epoch: 4 step: 546, loss is 0.0008816826739348471\n",
      "epoch: 4 step: 547, loss is 0.0009040622971951962\n",
      "epoch: 4 step: 548, loss is 0.0007459280896000564\n",
      "epoch: 4 step: 549, loss is 0.0005328303086571395\n",
      "epoch: 4 step: 550, loss is 0.0030053125228732824\n",
      "epoch: 4 step: 551, loss is 0.0009048580541275442\n",
      "epoch: 4 step: 552, loss is 0.0002592364908196032\n",
      "epoch: 4 step: 553, loss is 0.005887481849640608\n",
      "epoch: 4 step: 554, loss is 0.0011684364872053266\n",
      "epoch: 4 step: 555, loss is 0.000390246364986524\n",
      "epoch: 4 step: 556, loss is 0.00041917862836271524\n",
      "epoch: 4 step: 557, loss is 0.000725896330550313\n",
      "epoch: 4 step: 558, loss is 0.0008404584368690848\n",
      "epoch: 4 step: 559, loss is 0.0008986298926174641\n",
      "epoch: 4 step: 560, loss is 0.0018168381648138165\n",
      "epoch: 4 step: 561, loss is 0.0006578328902833164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 562, loss is 0.00028150773141533136\n",
      "epoch: 4 step: 563, loss is 0.0002577918639872223\n",
      "epoch: 4 step: 564, loss is 0.0002096847165375948\n",
      "epoch: 4 step: 565, loss is 0.000577963306568563\n",
      "epoch: 4 step: 566, loss is 0.0005261434707790613\n",
      "epoch: 4 step: 567, loss is 0.00018621466006152332\n",
      "epoch: 4 step: 568, loss is 0.0006171207060106099\n",
      "epoch: 4 step: 569, loss is 0.0003194824676029384\n",
      "epoch: 4 step: 570, loss is 0.00014941765402909368\n",
      "epoch: 4 step: 571, loss is 0.00024215105804614723\n",
      "epoch: 4 step: 572, loss is 0.00029553190688602626\n",
      "epoch: 4 step: 573, loss is 0.000577410391997546\n",
      "epoch: 4 step: 574, loss is 0.0002791588776744902\n",
      "epoch: 4 step: 575, loss is 0.0008016073843464255\n",
      "epoch: 4 step: 576, loss is 0.0005238247686065733\n",
      "epoch: 4 step: 577, loss is 0.00024396096705459058\n",
      "epoch: 4 step: 578, loss is 0.0003717077779583633\n",
      "epoch: 4 step: 579, loss is 0.0006567423115484416\n",
      "epoch: 4 step: 580, loss is 0.00036913849180564284\n",
      "epoch: 4 step: 581, loss is 0.0008865030249580741\n",
      "epoch: 4 step: 582, loss is 0.0002542596193961799\n",
      "epoch: 4 step: 583, loss is 0.0005199591396376491\n",
      "epoch: 4 step: 584, loss is 0.0011256039142608643\n",
      "epoch: 4 step: 585, loss is 0.00036436133086681366\n",
      "epoch: 4 step: 586, loss is 0.000633000920061022\n",
      "epoch: 4 step: 587, loss is 0.0006434758543036878\n",
      "epoch: 4 step: 588, loss is 0.0007099087233655155\n",
      "epoch: 4 step: 589, loss is 0.0007568645523861051\n",
      "epoch: 4 step: 590, loss is 0.000607334659434855\n",
      "epoch: 4 step: 591, loss is 0.00041951093589887023\n",
      "epoch: 4 step: 592, loss is 0.0005534751107916236\n",
      "epoch: 4 step: 593, loss is 0.0012014185776934028\n",
      "epoch: 4 step: 594, loss is 0.00046655454207211733\n",
      "epoch: 4 step: 595, loss is 0.0007799403974786401\n",
      "epoch: 4 step: 596, loss is 0.001026446232572198\n",
      "Train epoch time: 32253.729 ms, per step time: 54.117 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "# 训练模型，设定训练的轮数、数据集和回调函数\n",
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d18da77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # 将输入的句子转化为小写并去掉头尾的空格\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    # 将句子按空格分词\n",
    "    sentence = sentence.split(' ')\n",
    "    # 获取配置的句子最大长度\n",
    "    maxlen = cfg.word_len\n",
    "    # 初始化句子向量，长度为最大长度，所有元素都为0\n",
    "    vector = [0]*maxlen\n",
    "    # 遍历分词后的句子，如果词在词典中，则将词向量的对应位置设为该词在词典中的位置\n",
    "    for index, word in enumerate(sentence):\n",
    "        # 如果词的位置超过了最大长度，则退出循环\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        # 如果词不在词典中，则打印警告信息\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        # 如果词在词典中，则将词向量的对应位置设为该词在词典中的位置\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    # 将处理好的句子向量返回\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# 使用训练好的模型进行推理的函数\n",
    "def inference(review_en):\n",
    "    # 首先对输入的评论进行预处理，转化为词向量\n",
    "    review_en = preprocess(review_en)\n",
    "    # 将词向量转化为张量，并添加一个维度作为批处理的维度\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    # 将输入的张量通过模型，得到输出结果\n",
    "    output = net(input_en)\n",
    "    # 对输出结果进行处理，如果输出结果的最大值所在的位置为1，则认为是正面评论，否则是负面评论\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4293d77b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so wonderful\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9780db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb61f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0b72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
